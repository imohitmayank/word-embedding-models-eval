{"cells":[{"cell_type":"markdown","source":"# Word2Vec models comparison\n\nWe compare the different models of word2vec against different intrinsic word embeddings tasks.","metadata":{"cell_id":"00001-041ba339-031d-426b-8697-9dda6244991a","tags":[]}},{"cell_type":"markdown","source":"### Import and load datasets","metadata":{"cell_id":"00001-7d1cb22c-84e3-419d-8987-607100f4798d","tags":[]}},{"cell_type":"code","metadata":{"cell_id":"00002-3b9bb0a0-e60c-4aee-8287-c11f6e167fc0","tags":[],"output_cleared":false},"source":"# imports\nimport pickle\nimport random\n!pip install ray\nimport ray\nimport xml.etree.ElementTree as ET\n# ray.init()\n!pip install xlrd\n!pip install nltk\nimport nltk\nnltk.download('wordnet')\nimport glob\n!pip install tqdm\nfrom tqdm import tqdm\nimport pandas as pd\n!pip install gensim\nfrom gensim.models import KeyedVectors\nfrom gensim.models.word2vec import Word2Vec\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import Counter","outputs":[{"name":"stdout","text":"Collecting ray\n  Downloading ray-0.8.7-cp37-cp37m-manylinux1_x86_64.whl (22.0 MB)\n\u001b[K     |████████████████████████████████| 22.0 MB 15.4 MB/s eta 0:00:01\n\u001b[?25hCollecting filelock\n  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\nCollecting aiohttp\n  Downloading aiohttp-3.6.2-cp37-cp37m-manylinux1_x86_64.whl (1.2 MB)\n\u001b[K     |████████████████████████████████| 1.2 MB 32.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /opt/venv/lib/python3.7/site-packages (from ray) (0.8.0)\nCollecting aioredis\n  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n\u001b[K     |████████████████████████████████| 65 kB 2.8 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/venv/lib/python3.7/site-packages (from ray) (2.24.0)\nRequirement already satisfied: jsonschema in /opt/venv/lib/python3.7/site-packages (from ray) (3.2.0)\nRequirement already satisfied: numpy>=1.16 in /opt/venv/lib/python3.7/site-packages (from ray) (1.19.0)\nCollecting opencensus\n  Downloading opencensus-0.7.10-py2.py3-none-any.whl (126 kB)\n\u001b[K     |████████████████████████████████| 126 kB 27.8 MB/s eta 0:00:01\n\u001b[?25hCollecting google\n  Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)\n\u001b[K     |████████████████████████████████| 45 kB 1.3 MB/s  eta 0:00:01\n\u001b[?25hCollecting msgpack<2.0.0,>=1.0.0\n  Downloading msgpack-1.0.0-cp37-cp37m-manylinux1_x86_64.whl (275 kB)\n\u001b[K     |████████████████████████████████| 275 kB 17.3 MB/s eta 0:00:01\n\u001b[?25hCollecting gpustat\n  Downloading gpustat-0.6.0.tar.gz (78 kB)\n\u001b[K     |████████████████████████████████| 78 kB 3.2 MB/s  eta 0:00:01\n\u001b[?25hCollecting click>=7.0\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n\u001b[K     |████████████████████████████████| 82 kB 668 kB/s  eta 0:00:01\n\u001b[?25hCollecting colorama\n  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\nCollecting grpcio>=1.28.1\n  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n\u001b[K     |████████████████████████████████| 3.8 MB 22.4 MB/s eta 0:00:01\n\u001b[?25hCollecting redis<3.5.0,>=3.3.2\n  Downloading redis-3.4.1-py2.py3-none-any.whl (71 kB)\n\u001b[K     |████████████████████████████████| 71 kB 5.2 MB/s  eta 0:00:01\n\u001b[?25hCollecting colorful\n  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n\u001b[K     |████████████████████████████████| 201 kB 35.0 MB/s eta 0:00:01\n\u001b[?25hCollecting protobuf>=3.8.0\n  Downloading protobuf-3.13.0-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n\u001b[K     |████████████████████████████████| 1.3 MB 33.1 MB/s eta 0:00:01\n\u001b[?25hCollecting py-spy>=0.2.0\n  Downloading py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9 MB)\n\u001b[K     |████████████████████████████████| 2.9 MB 24.3 MB/s eta 0:00:01\n\u001b[?25hCollecting pyyaml\n  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n\u001b[K     |████████████████████████████████| 269 kB 33.2 MB/s eta 0:00:01\n\u001b[?25hCollecting yarl<2.0,>=1.0\n  Downloading yarl-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (258 kB)\n\u001b[K     |████████████████████████████████| 258 kB 32.7 MB/s eta 0:00:01\n\u001b[?25hCollecting async-timeout<4.0,>=3.0\n  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\nRequirement already satisfied: chardet<4.0,>=2.0 in /opt/venv/lib/python3.7/site-packages (from aiohttp->ray) (3.0.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/venv/lib/python3.7/site-packages (from aiohttp->ray) (19.3.0)\nCollecting multidict<5.0,>=4.5\n  Downloading multidict-4.7.6-cp37-cp37m-manylinux1_x86_64.whl (149 kB)\n\u001b[K     |████████████████████████████████| 149 kB 40.0 MB/s eta 0:00:01\n\u001b[?25hCollecting hiredis\n  Downloading hiredis-1.1.0-cp37-cp37m-manylinux2010_x86_64.whl (62 kB)\n\u001b[K     |████████████████████████████████| 62 kB 710 kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->ray) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->ray) (1.25.9)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->ray) (2.10)\nRequirement already satisfied: pyrsistent>=0.14.0 in /opt/venv/lib/python3.7/site-packages (from jsonschema->ray) (0.16.0)\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/venv/lib/python3.7/site-packages (from jsonschema->ray) (1.7.0)\nRequirement already satisfied: six>=1.11.0 in /opt/venv/lib/python3.7/site-packages (from jsonschema->ray) (1.15.0)\nRequirement already satisfied: setuptools in /opt/venv/lib/python3.7/site-packages (from jsonschema->ray) (47.3.1)\nCollecting google-api-core<2.0.0,>=1.0.0\n  Downloading google_api_core-1.22.2-py2.py3-none-any.whl (91 kB)\n\u001b[K     |████████████████████████████████| 91 kB 7.7 MB/s  eta 0:00:01\n\u001b[?25hCollecting opencensus-context==0.1.1\n  Downloading opencensus_context-0.1.1-py2.py3-none-any.whl (4.4 kB)\nCollecting beautifulsoup4\n  Downloading beautifulsoup4-4.9.1-py3-none-any.whl (115 kB)\n\u001b[K     |████████████████████████████████| 115 kB 38.2 MB/s eta 0:00:01\n\u001b[?25hCollecting nvidia-ml-py3>=7.352.0\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\nCollecting psutil\n  Downloading psutil-5.7.2.tar.gz (460 kB)\n\u001b[K     |████████████████████████████████| 460 kB 17.1 MB/s eta 0:00:01\n\u001b[?25hCollecting blessings>=1.6\n  Downloading blessings-1.7-py3-none-any.whl (18 kB)\nCollecting typing-extensions>=3.7.4; python_version < \"3.8\"\n  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\nRequirement already satisfied: zipp>=0.5 in /opt/venv/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema->ray) (3.1.0)\nCollecting google-auth<2.0dev,>=1.21.1\n  Downloading google_auth-1.21.1-py2.py3-none-any.whl (93 kB)\n\u001b[K     |████████████████████████████████| 93 kB 896 kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: pytz in /opt/venv/lib/python3.7/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (2020.1)\nCollecting googleapis-common-protos<2.0dev,>=1.6.0\n  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n\u001b[K     |████████████████████████████████| 100 kB 7.8 MB/s eta 0:00:01\n\u001b[?25hCollecting soupsieve>1.2\n  Downloading soupsieve-2.0.1-py3-none-any.whl (32 kB)\nCollecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n\u001b[K     |████████████████████████████████| 47 kB 3.8 MB/s  eta 0:00:01\n\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n\u001b[K     |████████████████████████████████| 155 kB 36.4 MB/s eta 0:00:01\n\u001b[?25hCollecting pyasn1>=0.1.3\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n\u001b[K     |████████████████████████████████| 77 kB 4.9 MB/s  eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: gpustat, pyyaml, nvidia-ml-py3, psutil\n  Building wheel for gpustat (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=e1af743bcae4974b1b084a12593ed8e6e98ae3aaf76dad41fdef0477445351ad\n  Stored in directory: /home/jovyan/.cache/pip/wheels/e6/67/af/f1ad15974b8fd95f59a63dbf854483ebe5c7a46a93930798b8\n  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44619 sha256=394937477a4440702d4882f387f7b2d52bbb83fd09f29d760a87662899f1ba98\n  Stored in directory: /home/jovyan/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19189 sha256=e2e8769cfe6adf9bfdb676c4832c4c6ad9c05093b4f7714ca1c82ca628e3b1a1\n  Stored in directory: /home/jovyan/.cache/pip/wheels/df/99/da/c34f202dc8fd1dffd35e0ecf1a7d7f8374ca05fbcbaf974b83\n  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for psutil: filename=psutil-5.7.2-cp37-cp37m-linux_x86_64.whl size=282620 sha256=702ba81762b30cded0a74f4f74caaf05b948bbd962e85fb9c607769a2eb16246\n  Stored in directory: /home/jovyan/.cache/pip/wheels/2d/43/97/00701864a7bee6d9e1a52dd682537dcbf1d013d0e2e6f0c1f1\nSuccessfully built gpustat pyyaml nvidia-ml-py3 psutil\nInstalling collected packages: filelock, typing-extensions, multidict, yarl, async-timeout, aiohttp, hiredis, aioredis, protobuf, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, googleapis-common-protos, google-api-core, opencensus-context, opencensus, soupsieve, beautifulsoup4, google, msgpack, nvidia-ml-py3, psutil, blessings, gpustat, click, colorama, grpcio, redis, colorful, py-spy, pyyaml, ray\nSuccessfully installed aiohttp-3.6.2 aioredis-1.3.1 async-timeout-3.0.1 beautifulsoup4-4.9.1 blessings-1.7 cachetools-4.1.1 click-7.1.2 colorama-0.4.3 colorful-0.5.4 filelock-3.0.12 google-3.0.0 google-api-core-1.22.2 google-auth-1.21.1 googleapis-common-protos-1.52.0 gpustat-0.6.0 grpcio-1.32.0 hiredis-1.1.0 msgpack-1.0.0 multidict-4.7.6 nvidia-ml-py3-7.352.0 opencensus-0.7.10 opencensus-context-0.1.1 protobuf-3.13.0 psutil-5.7.2 py-spy-0.3.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyyaml-5.3.1 ray-0.8.7 redis-3.4.1 rsa-4.6 soupsieve-2.0.1 typing-extensions-3.7.4.3 yarl-1.5.1\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nCollecting xlrd\n  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n\u001b[K     |████████████████████████████████| 103 kB 3.5 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: xlrd\nSuccessfully installed xlrd-1.2.0\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nCollecting nltk\n  Downloading nltk-3.5.zip (1.4 MB)\n\u001b[K     |████████████████████████████████| 1.4 MB 3.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: click in /opt/venv/lib/python3.7/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: joblib in /opt/venv/lib/python3.7/site-packages (from nltk) (0.16.0)\nCollecting regex\n  Downloading regex-2020.7.14-cp37-cp37m-manylinux2010_x86_64.whl (660 kB)\n\u001b[K     |████████████████████████████████| 660 kB 28.8 MB/s eta 0:00:01\n\u001b[?25hCollecting tqdm\n  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n\u001b[K     |████████████████████████████████| 68 kB 5.5 MB/s  eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: nltk\n  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434674 sha256=8203f815a3d5de5e41d5fdb7fbf53af995e4b6a29b1733963e8be2bf6523eb9a\n  Stored in directory: /home/jovyan/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\nSuccessfully built nltk\nInstalling collected packages: regex, tqdm, nltk\nSuccessfully installed nltk-3.5 regex-2020.7.14 tqdm-4.48.2\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\nRequirement already satisfied: tqdm in /opt/venv/lib/python3.7/site-packages (4.48.2)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nCollecting gensim\n  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n\u001b[K     |████████████████████████████████| 24.2 MB 163 kB/s eta 0:00:01��██████████████▋   | 21.6 MB 3.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.5.1)\nRequirement already satisfied: six>=1.5.0 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.15.0)\nCollecting smart-open>=1.8.1\n  Downloading smart_open-2.1.1.tar.gz (111 kB)\n\u001b[K     |████████████████████████████████| 111 kB 36.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.19.0)\nRequirement already satisfied: requests in /opt/venv/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\nCollecting boto\n  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n\u001b[K     |████████████████████████████████| 1.4 MB 31.9 MB/s eta 0:00:01\n\u001b[?25hCollecting boto3\n  Downloading boto3-1.14.60-py2.py3-none-any.whl (129 kB)\n\u001b[K     |████████████████████████████████| 129 kB 35.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\nCollecting botocore<1.18.0,>=1.17.60\n  Downloading botocore-1.17.60-py2.py3-none-any.whl (6.6 MB)\n\u001b[K     |████████████████████████████████| 6.6 MB 31.5 MB/s eta 0:00:01\n\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\nCollecting s3transfer<0.4.0,>=0.3.0\n  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n\u001b[K     |████████████████████████████████| 69 kB 5.1 MB/s  eta 0:00:01\n\u001b[?25hCollecting docutils<0.16,>=0.10\n  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n\u001b[K     |████████████████████████████████| 547 kB 33.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/venv/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.60->boto3->smart-open>=1.8.1->gensim) (2.8.1)\nBuilding wheels for collected packages: smart-open\n  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.1-py3-none-any.whl size=112413 sha256=29d25d73718a6c7cb4198a8ec9ad5847efe008999681ee8b3f77389333a0d7ff\n  Stored in directory: /home/jovyan/.cache/pip/wheels/d5/75/1d/d3da0d094f5e2ea61bddb9cb8cd4bd3ab9fb648e7269620855\nSuccessfully built smart-open\nInstalling collected packages: boto, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\nSuccessfully installed boto-2.49.0 boto3-1.14.60 botocore-1.17.60 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.1\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"cell_id":"00003-5bb28d92-0786-44ba-865d-7b3cef2d7bd6","output_cleared":false},"source":"# lemmatizer - noun lemma -- https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word\ndef lemma(word): return nltk.stem.WordNetLemmatizer().lemmatize(word)\n\n# preprocss the word - lowercase and lemma\ndef pre(word): return lemma(word.lower())\n\ndef check_word(word): return \" \" not in word and \".\" not in word and \"-\" not in word and \"/\" not in word","outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load similarity/relatedness dataset","metadata":{"cell_id":"00003-31060c96-802e-45cc-a527-ef28db976abe","tags":[]}},{"cell_type":"code","metadata":{"cell_id":"00003-498c0693-1ca7-4c9e-ad7d-b58ca2b2f18e","tags":[]},"source":"# load the files\ndef load_similarity_datasets():\n    \"\"\"Load all (13) datasets which can be used to test word interchangeable similarity\n    \"\"\"\n    sim_data = {}\n    for file_path in glob.glob(\"../data/word-sim/*\"):\n        file_name = file_path[17:].replace(\".txt\", \"\")\n        print(file_name)\n        try:\n            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n            df.columns = ['word_1', 'word_2', 'similarity_score']\n        except:\n            df = pd.read_csv(file_path, sep=\" \", header=None)\n            df.columns = ['word_1', 'word_2', 'similarity_score']\n        sim_data[file_name] = df\n    return sim_data\n\n# load similarity datasets\nsimilarity_datasets = load_similarity_datasets()","execution_count":null,"outputs":[{"name":"stdout","text":"EN-VERB-143\nEN-SimVerb-3500\nEN-RG-65\nEN-RW-STANFORD\nEN-MTurk-771\nEN-MEN-TR-3k\nEN-MC-30\nEN-MTurk-287\nEN-SIMLEX-999\nEN-WS-353-REL\nEN-YP-130\nEN-WS-353-ALL\nEN-WS-353-SIM\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load association datasets","metadata":{"cell_id":"00004-590b2013-594e-47e2-b9fb-dbe444415ca6","tags":[]}},{"cell_type":"code","metadata":{"cell_id":"00007-4f50110c-4d40-46d0-b8bd-143d0b0d36bc","tags":[]},"source":"def prepare_r123_strength_table(cue_data, cue_name):\n    # calculate R123 strength\n    responses = cue_data.loc[:, ['R1', 'R2', 'R3']].values.reshape(1, -1)[0]\n    responses = [pre(str(x)) for x in responses if str(x) != \"nan\" if \"-\" not in str(x)]\n    responses = pd.DataFrame.from_dict(Counter(responses), orient='index').reset_index()\n    responses.columns = ['response', 'R123']\n    responses.loc[:, 'N'] = responses['R123'].sum()\n    responses.loc[:, 'R123.Str'] = responses['R123'] / responses['N']\n    responses.loc[:, 'cue'] = pre(str(cue_name))\n    return responses\n\ndef prepare_swow_8500(conf_filter=0.1):\n    \"\"\"Return swow words with r123.str >= conf_filter (default:0.1 leads to 8270 unqiue cues)\n    \"\"\"\n    # handle swow-8500\n    data = pd.read_excel(\"../data/association/swow-8500.xlsx\")\n    swow_8500 = []\n    for cue_name, cue_data in tqdm(data.groupby(['cue']), position=0, leave=True, desc=\"Loading SWOW\"):\n        if \" \" in str(cue_name):\n            continue\n        swow_8500.append(prepare_r123_strength_table(cue_data, cue_name))\n    swow_8500 = pd.concat(swow_8500)\n    swow_8500 = swow_8500.loc[swow_8500['R123.Str']>=conf_filter]\n    return swow_8500\n\n# swow_8500_df = prepare_swow_8500(df)\n\ndef clean_eat_dataset(data, conf_filter=0.1):\n    data.loc[:, 'occ_conf'] = data.loc[:, 'occ_conf'].astype(float)\n    data.loc[:, 'occ_count'] = data.loc[:, 'occ_count'].astype(int)\n    data = data.query(f\"occ_conf >= {conf_filter}\")\n    return data\n\ndef prepare_eat_dataset(conf_filter=0.1):\n    \"\"\"http://rali.iro.umontreal.ca/rali/?q=en/Textual%20Resources/EAT\n    \"\"\"\n    tree = ET.parse('../data/association/eat-stimulus-response.xml')\n    root = tree.getroot()\n    eat_table = []\n    for stimulus in tqdm(root.findall(\"stimulus\"), position=0, leave=True, desc=\"Loading EAT\"):\n        stimulus_word = stimulus.attrib['word']\n        if check_word(stimulus_word):\n            for res in stimulus.findall(\"response\"):\n                res_word = res.attrib['word']\n                if check_word(res_word):\n                    eat_table.append({'cue': pre(stimulus_word), 'response': pre(res_word), \n                               'occ_count': res.attrib['n'], 'occ_conf': res.attrib['r']})\n    eat_table = pd.DataFrame.from_dict(eat_table)\n    return clean_eat_dataset(eat_table, conf_filter=0.1)\n\ndef load_association_dataset():\n    return {\"swow8500\": prepare_swow_8500(), \"eat\": prepare_eat_dataset()}\n\nassociation_datasets = load_association_dataset()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-faf0e6eb-76a3-49cf-bab1-f2247ea8c655"},"source":"# # association_datasets['eat']['occ_count'].astype(int)\n# eat_dataset = []\n# for cue_name, cue_data in association_datasets['eat'].groupby(['cue']):\n#     cue_data_copy = cue_data.copy()\n#     cue_data_copy.loc[:, 'occ_conf'] = cue_data_copy.loc[:, 'occ_conf'].astype(float)\n#     cue_data_copy.loc[:, 'occ_count'] = cue_data_copy.loc[:, 'occ_count'].astype(int)\n#     cue_data_copy.loc[:, 'occ_conf'] = cue_data_copy['occ_count']/cue_data_copy['occ_count'].sum()\n#     eat_dataset.append(cue_data_copy)\n# eat_dataset = pd.concat(eat_dataset)\n\nx.query(\"occ_conf >= 0.1\")['cue'].nunique()","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":61,"data":{"text/plain":"6673"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-41ff796f-ec5d-4b83-8e39-4fcc69652cd9"},"source":"# association_datasets['eat']['occ_conf'].astype(float).describe()\n# x = clean_eat_dataset(association_datasets['eat'].copy())\nassociation_datasets['eat']['cue'].nunique()","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":62,"data":{"text/plain":"7182"},"metadata":{}}]},{"cell_type":"markdown","source":"## Load Analogy datasets","metadata":{"cell_id":"00008-40b22d64-e52f-438e-a078-e032dd8565ab"}},{"cell_type":"code","metadata":{"cell_id":"00009-cc482826-3fea-4a95-b456-492ff9236637"},"source":"def load_google_analogy():\n    google_analogy={}\n    with open(\"../data/analogy/google_analogy_set.txt\", 'r') as f:\n        for line in f:\n            line = line.replace(\"\\n\", \"\")\n            if \":\" in line: # its a title\n                title = line[2:]\n                google_analogy[title] = []\n            else:\n                analogy = [pre(x) for x in line.split() if check_word(x)]\n                if len(analogy) == 4:\n                    google_analogy[title].append(analogy)\n    return google_analogy\n\n# x = load_google_analogy()\n\n\ndef load_bats_analogy():\n    random.seed(0)\n    file_analogy = []\n    for section_path in glob.glob(\"../data/analogy/BATS_3.0/[0-9]*\"):\n        if \"Inflectional_morphology\" in section_path:\n            continue\n        section_name = section_path[10:]\n        for file_path in glob.glob(section_path+\"/*\"):\n            file_name = file_path.replace(section_path, \"\")\n            file_analogy_prefix = []\n            with open(file_path, 'r') as f:\n                for line in f:\n                    analogy_prefix = [pre(x) for x in line.split() if check_word(x)]\n                    if len(set(analogy_prefix)) == 2:\n                        file_analogy_prefix.append(analogy_prefix)\n            random_choices=63\n            if len(file_analogy_prefix) > random_choices*1:\n                for _ in range(random_choices):\n                    a, b = random.sample(file_analogy_prefix, 2)\n                    a, b = a.copy(), b.copy()\n                    a+=b\n                    file_analogy.append(a)\n    file_analogy = [x for x in file_analogy if len(set(x)) == 4]\n    return file_analogy\n\nfile_analogy = load_bats_analogy()\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test dataset file","metadata":{"tags":[],"cell_id":"00013-7bb233b0-4b5a-4bc5-b123-d415bcc9c12b"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00014-949b7faa-c2ea-416c-b864-88b87f375f3d","output_cleared":false},"source":"import pickle\nwith open(\"../data/all_datasets.pickle\", 'rb') as f:\n    all_dataset = pickle.load(f)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00015-7d5c48f7-1016-4de9-a670-eff9309355f2","output_cleared":false},"source":"# all_dataset['association_datasets']['eat']['cue'].nunique()\nlen(all_dataset['analogy_datasets']['google_analogy'])#.keys()","outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"994"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00014-21021de1-20f7-40cd-a15a-76a9a4e7e67a"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"3bec70f7-4277-4683-a3e5-692e28462d6a","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}}}