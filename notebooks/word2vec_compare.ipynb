{"cells":[{"cell_type":"markdown","source":"# Word2Vec models comparison\n\nWe compare the different models of word2vec against different intrinsic word embeddings tasks.\n\n**BLUNDER: all 200 and 300 D models are not trained on leema text**","metadata":{"cell_id":"00000-3e5dd85b-a83f-41e3-81ec-63465c987242","tags":[]}},{"cell_type":"markdown","source":"### Import and load datasets","metadata":{"cell_id":"00001-1d47f6bc-3830-48d3-969f-6f724ac6e2ca","tags":[]}},{"cell_type":"code","metadata":{"cell_id":"00002-9e2b57be-6c76-439b-9f0e-24ee9215ace3","tags":[]},"source":"# imports\n!pip install ray[tune]\nimport ray\nfrom ray.tune.utils import pin_in_object_store, get_pinned_object\nray.init(ignore_reinit_error=True)\nimport pickle\nimport time\nfrom numpy.linalg import norm\nfrom scipy.spatial.distance import cosine, cdist\nimport base64\n!pip install telepot\nimport telepot\n\n!pip install nltk\nimport nltk\nnltk.download('wordnet')\nimport glob\n!pip install tqdm\nfrom tqdm import tqdm\nimport pandas as pd\n!pip install gensim\nfrom gensim.models import KeyedVectors\nfrom gensim.models.word2vec import Word2Vec\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity","execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ray[tune] in /opt/venv/lib/python3.7/site-packages (0.8.7)\nRequirement already satisfied: py-spy>=0.2.0 in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (0.3.3)\nRequirement already satisfied: gpustat in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (0.6.0)\nRequirement already satisfied: opencensus in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (0.7.10)\nRequirement already satisfied: aioredis in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (1.3.1)\nRequirement already satisfied: colorama in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (0.4.3)\nRequirement already satisfied: aiohttp in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (3.6.2)\nRequirement already satisfied: jsonschema in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (3.2.0)\nRequirement already satisfied: redis<3.5.0,>=3.3.2 in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (3.4.1)\nRequirement already satisfied: numpy>=1.16 in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (1.19.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (1.0.0)\nRequirement already satisfied: prometheus-client>=0.7.1 in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (0.8.0)\nRequirement already satisfied: grpcio>=1.28.1 in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (1.31.0)\nRequirement already satisfied: requests in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (2.24.0)\nRequirement already satisfied: filelock in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (3.0.12)\nRequirement already satisfied: google in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (3.0.0)\nRequirement already satisfied: pyyaml in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (5.3.1)\nRequirement already satisfied: protobuf>=3.8.0 in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (3.13.0)\nRequirement already satisfied: colorful in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (0.5.4)\nRequirement already satisfied: click>=7.0 in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (7.1.2)\nRequirement already satisfied: tensorboardX; extra == \"tune\" in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (2.1)\nRequirement already satisfied: pandas; extra == \"tune\" in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (1.0.5)\nRequirement already satisfied: tabulate; extra == \"tune\" in /opt/venv/lib/python3.7/site-packages (from ray[tune]) (0.8.7)\nRequirement already satisfied: six>=1.7 in /opt/venv/lib/python3.7/site-packages (from gpustat->ray[tune]) (1.15.0)\nRequirement already satisfied: blessings>=1.6 in /opt/venv/lib/python3.7/site-packages (from gpustat->ray[tune]) (1.7)\nRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /opt/venv/lib/python3.7/site-packages (from gpustat->ray[tune]) (7.352.0)\nRequirement already satisfied: psutil in /opt/venv/lib/python3.7/site-packages (from gpustat->ray[tune]) (5.7.2)\nRequirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /opt/venv/lib/python3.7/site-packages (from opencensus->ray[tune]) (1.22.1)\nRequirement already satisfied: opencensus-context==0.1.1 in /opt/venv/lib/python3.7/site-packages (from opencensus->ray[tune]) (0.1.1)\nRequirement already satisfied: hiredis in /opt/venv/lib/python3.7/site-packages (from aioredis->ray[tune]) (1.1.0)\nRequirement already satisfied: async-timeout in /opt/venv/lib/python3.7/site-packages (from aioredis->ray[tune]) (3.0.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/venv/lib/python3.7/site-packages (from aiohttp->ray[tune]) (1.5.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/venv/lib/python3.7/site-packages (from aiohttp->ray[tune]) (19.3.0)\nRequirement already satisfied: chardet<4.0,>=2.0 in /opt/venv/lib/python3.7/site-packages (from aiohttp->ray[tune]) (3.0.4)\nRequirement already satisfied: multidict<5.0,>=4.5 in /opt/venv/lib/python3.7/site-packages (from aiohttp->ray[tune]) (4.7.6)\nRequirement already satisfied: pyrsistent>=0.14.0 in /opt/venv/lib/python3.7/site-packages (from jsonschema->ray[tune]) (0.16.0)\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/venv/lib/python3.7/site-packages (from jsonschema->ray[tune]) (1.7.0)\nRequirement already satisfied: setuptools in /opt/venv/lib/python3.7/site-packages (from jsonschema->ray[tune]) (47.3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->ray[tune]) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->ray[tune]) (1.25.9)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->ray[tune]) (2.10)\nRequirement already satisfied: beautifulsoup4 in /opt/venv/lib/python3.7/site-packages (from google->ray[tune]) (4.9.1)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/venv/lib/python3.7/site-packages (from pandas; extra == \"tune\"->ray[tune]) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/venv/lib/python3.7/site-packages (from pandas; extra == \"tune\"->ray[tune]) (2020.1)\nRequirement already satisfied: google-auth<2.0dev,>=1.19.1 in /opt/venv/lib/python3.7/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (1.21.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/venv/lib/python3.7/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (1.52.0)\nRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /opt/venv/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->ray[tune]) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/venv/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema->ray[tune]) (3.1.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/venv/lib/python3.7/site-packages (from beautifulsoup4->google->ray[tune]) (2.0.1)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/venv/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.19.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (4.1.1)\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/venv/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.19.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (4.6)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/venv/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.19.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (0.2.8)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/venv/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2.0dev,>=1.19.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (0.4.8)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n2020-09-01 20:06:58,639\tINFO resource_spec.py:231 -- Starting Ray with 4.05 GiB memory available for workers and up to 2.03 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n2020-09-01 20:07:02,863\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n2020-09-01 20:07:02,927\tWARNING services.py:1567 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n2020-09-01 20:07:03,260\tWARNING services.py:1567 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\nRequirement already satisfied: telepot in /opt/venv/lib/python3.7/site-packages (12.7)\nRequirement already satisfied: urllib3>=1.9.1 in /opt/venv/lib/python3.7/site-packages (from telepot) (1.25.9)\nRequirement already satisfied: aiohttp>=3.0.0 in /opt/venv/lib/python3.7/site-packages (from telepot) (3.6.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/venv/lib/python3.7/site-packages (from aiohttp>=3.0.0->telepot) (1.5.1)\nRequirement already satisfied: chardet<4.0,>=2.0 in /opt/venv/lib/python3.7/site-packages (from aiohttp>=3.0.0->telepot) (3.0.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/venv/lib/python3.7/site-packages (from aiohttp>=3.0.0->telepot) (19.3.0)\nRequirement already satisfied: multidict<5.0,>=4.5 in /opt/venv/lib/python3.7/site-packages (from aiohttp>=3.0.0->telepot) (4.7.6)\nRequirement already satisfied: async-timeout<4.0,>=3.0 in /opt/venv/lib/python3.7/site-packages (from aiohttp>=3.0.0->telepot) (3.0.1)\nRequirement already satisfied: idna>=2.0 in /opt/venv/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.0.0->telepot) (2.10)\nRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /opt/venv/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.0.0->telepot) (3.7.4.3)\n2020-09-01 20:07:42,995\tWARNING worker.py:1134 -- The dashboard on node p-302a6336-d27e-43af-93d1-276242f3b519 failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/venv/lib/python3.7/site-packages/ray/dashboard/dashboard.py\", line 961, in <module>\n    dashboard.run()\n  File \"/opt/venv/lib/python3.7/site-packages/ray/dashboard/dashboard.py\", line 576, in run\n    aiohttp.web.run_app(self.app, host=self.host, port=self.port)\n  File \"/opt/venv/lib/python3.7/site-packages/aiohttp/web.py\", line 433, in run_app\n    reuse_port=reuse_port))\n  File \"/usr/local/lib/python3.7/asyncio/base_events.py\", line 584, in run_until_complete\n    return future.result()\n  File \"/opt/venv/lib/python3.7/site-packages/aiohttp/web.py\", line 359, in _run_app\n    await site.start()\n  File \"/opt/venv/lib/python3.7/site-packages/aiohttp/web_runner.py\", line 104, in start\n    reuse_port=self._reuse_port)\n  File \"/usr/local/lib/python3.7/asyncio/base_events.py\", line 1378, in create_server\n    % (sa, err.strerror.lower())) from None\nOSError: [Errno 99] error while attempting to bind on address ('::1', 8265, 0, 0): cannot assign requested address\n\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: nltk in /opt/venv/lib/python3.7/site-packages (3.5)\nRequirement already satisfied: joblib in /opt/venv/lib/python3.7/site-packages (from nltk) (0.16.0)\nRequirement already satisfied: click in /opt/venv/lib/python3.7/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: regex in /opt/venv/lib/python3.7/site-packages (from nltk) (2020.7.14)\nRequirement already satisfied: tqdm in /opt/venv/lib/python3.7/site-packages (from nltk) (4.48.2)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\nRequirement already satisfied: tqdm in /opt/venv/lib/python3.7/site-packages (4.48.2)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: gensim in /opt/venv/lib/python3.7/site-packages (3.8.3)\nRequirement already satisfied: numpy>=1.11.3 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.19.0)\nRequirement already satisfied: six>=1.5.0 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.15.0)\nRequirement already satisfied: scipy>=0.18.1 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.5.1)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/venv/lib/python3.7/site-packages (from gensim) (2.1.1)\nRequirement already satisfied: requests in /opt/venv/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\nRequirement already satisfied: boto3 in /opt/venv/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.14.52)\nRequirement already satisfied: boto in /opt/venv/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/venv/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/venv/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\nRequirement already satisfied: botocore<1.18.0,>=1.17.52 in /opt/venv/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.52)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/venv/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.52->boto3->smart-open>=1.8.1->gensim) (0.15.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/venv/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.52->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-2f225dfe-99d1-438d-a35f-5ff619480095"},"source":"## Load telepot\ndef send_telegram_message(message):\n    token = '1361671158:AAF9jfW_fT0aF0zHwHtpOaUEB9CPYhmyew8'\n    TelegramBot = telepot.Bot(token)\n    TelegramBot.sendMessage(934022573, str(message))","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-d7473357-957f-4110-a0b9-b58274497464"},"source":"# send_telegram_message(\"test from deepnotes\")","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-2a8bf924-33fd-402a-b346-8a3fd2e34a3f"},"source":"with open (\"../data/all_datasets.pickle\", 'rb') as f:\n    all_datasets = pickle.load(f)","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-86a66fbc-424b-492b-a79a-b01e1dee6926"},"source":"# all([len(set(x))==4 for x in all_datasets['analogy_datasets']['bats_analogy']])\n# all_datasets['analogy_datasets']['bats_analogy'][:10]\n# for x , y in all_datasets['relatedness_datasets'].items():\n#     print(x, len(y))","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Load word2vec models","metadata":{"cell_id":"00003-7aa5267f-439e-4e0e-bce0-1c113f44ced3","tags":[]}},{"cell_type":"code","metadata":{"cell_id":"00004-b38d3970-71ec-445e-8d56-ee126cda621c","tags":[]},"source":"def nnorm(matrix):\n    \"norm each vector in the matrix\"\n    return (matrix.T/norm(matrix, axis=1)).T\n    \n# model = Word2Vec.load(\"../../../embeddings_lemma/word2vec_mc=10_iter=5_size=200_window=5_sg=0/word2vec_wikiEn20171001_millionSentences_mc=10_iter=5_size=200_window=5_sg=0\")\n# model.wv.vectors = nnorm(model.wv.vectors) \n# model.trainables.syn1neg = nnorm(model.trainables.syn1neg)\n","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Create Word2vec similarity computing method","metadata":{"cell_id":"00005-0d59b9b7-fd9e-4264-b365-928853de76d4","tags":[]}},{"cell_type":"code","metadata":{"cell_id":"00006-53360c62-c699-4129-99da-de30347538ce","tags":[]},"source":"def word2vec_get_index_by_word(word2vec_model, word):\n    \"\"\"Return the index of the word in the model\n    \"\"\"\n    return word2vec_model.wv.index2word.index(word)\n\ndef word2vec_get_word_by_index(word2vec_model, index):\n    \"\"\"Return the word by the provided index\n    \"\"\"\n    return word2vec_model.wv.index2word[index]\n\ndef word2vec_get_model_matrix_by_method(word2vec_model, method):\n    \"\"\"Return the source and compare matrix based on model and method selection\n    \"\"\"\n    input_weights = word2vec_model.wv.vectors\n    output_weights = word2vec_model.trainables.syn1neg\n    weights1, weights2 = None, None\n    if method==\"IN-IN\":\n        weights1, weights2 = input_weights, input_weights \n    elif method==\"IN-OUT\":\n        weights1, weights2 = input_weights, output_weights\n    elif method==\"OUT-IN\":\n        weights1, weights2 = output_weights, input_weights\n    elif method==\"OUT-OUT\":\n        weights1, weights2 = output_weights, output_weights\n    return weights1, weights2\n\ndef word2vec_find_top_similar_vectors_bw_matrix(vector, compare_matrix, source_word_index, index2word, top_n=5, no_self_similarity=True):\n    \"\"\"Find top n similar vectors in the compare matrix\n    \"\"\"\n    score = np.dot(vector, compare_matrix)[0]\n    if no_self_similarity:\n        score[source_word_index] = -1 # negate self-similarity\n    top_n_similar_words = np.argpartition(-score, top_n)[:top_n]\n    sorted_result = sorted([(index2word[index], score[index]) for index in top_n_similar_words], \n                key=lambda x: x[1], \n                reverse=True)\n    return sorted_result\n\ndef word2vec_find_top_similar_words(word2vec_model, source_word, method='IN-IN', top_n=5, no_self_similarity=True):\n    \"\"\"\n    Provided a word, find the top_n most similar from the model following the method\n    \"\"\"\n    score = []\n    source_word_index = word2vec_get_index_by_word(word2vec_model, source_word)\n    source_matrix, compare_matrix = word2vec_get_model_matrix_by_method(word2vec_model, method)\n    sorted_result = word2vec_find_top_similar_vectors_bw_matrix(\n            vector=source_matrix[source_word_index].reshape(1, -1),\n            compare_matrix=compare_matrix.T, \n            source_word_index = source_word_index,\n            index2word=word2vec_model.wv.index2word,\n            top_n=top_n, \n            no_self_similarity=no_self_similarity)\n    return sorted_result\n\ndef word2vec_find_similarity(word2vec_model, source_word, target_word, method=\"IN-IN\"):\n    \"\"\"Return the cosine similarity between two words based on the suggested method\n    \"\"\"\n    source_word_index = word2vec_get_index_by_word(word2vec_model, source_word)\n    target_word_index = word2vec_get_index_by_word(word2vec_model, target_word)\n    source_matrix, compare_matrix = word2vec_get_model_matrix_by_method(word2vec_model, method)\n    score = cosine_similarity(source_matrix[source_word_index].reshape(1, -1), \n                              compare_matrix[target_word_index].reshape(1, -1))[0]\n    return score\n\n# word2vec_find_similarity(model, \"car\", \"truck\", \"IN-OUT\")\n# word2vec_find_top_similar_words(model, \"car\", \"IN-IN\")\n# word2vec_find_top_similar_words(model, \"car\", \"IN-OUT\")\n\n# lemmatizer - noun lemma -- https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word\ndef lemma(word): return nltk.stem.WordNetLemmatizer().lemmatize(word)\n\n# preprocss the word - lowercase and lemma\ndef pre(word): return lemma(word.lower())","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-428591ef-cbf0-4dfd-842b-985a97e70458"},"source":"# word2vec_find_top_similar_words(get_pinned_object(model), 'car', method='IN-IN', top_n=10, no_self_similarity=True)","execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Test functions","metadata":{"cell_id":"00010-0ee6049a-6cce-4f29-b41d-db37bcd57547","tags":[]}},{"cell_type":"code","metadata":{"cell_id":"00011-8f52192d-aa1f-495d-89ad-5b52b2a358df","tags":[]},"source":"glob_w2v_models = ['../../../embeddings_lemma/word2vec_mc=10_iter=5_size=100_window=5_sg=0',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=100_window=5_sg=1',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=100_window=50_sg=0',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=100_window=50_sg=1',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=200_window=5_sg=0',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=200_window=5_sg=1',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=200_window=50_sg=0',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=200_window=50_sg=1',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=300_window=5_sg=0',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=300_window=5_sg=1',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=300_window=50_sg=0',\n                    '../../../embeddings_lemma/word2vec_mc=10_iter=5_size=300_window=50_sg=1']\n\n# @ray.remote\ndef compare_word2vec_model_with_relatedness_dataset(model, model_name, dataset_name, dataset):\n    missing_words = 0\n    score_table = []\n    for row in dataset.to_dict(orient=\"records\"):\n        methods = [\"IN-IN\", \"IN-OUT\", \"OUT-IN\", \"OUT-OUT\"]\n        for method in methods:\n            try:\n                sim_score = word2vec_find_similarity(model, pre(row['word_1']), pre(row['word_2']), method)[0]\n            except:\n                sim_score = None\n                missing_words += 1\n            row[f\"{model_name}_{method}\"] = sim_score\n        score_table.append(row)\n    score_table = pd.DataFrame.from_dict(score_table)\n    score_table = score_table.dropna().corr(\"pearson\")[['similarity_score']].tail(4)\n    score_table.columns = [dataset_name]\n    missing_words = missing_words/len(methods)\n    return score_table, dataset_name, missing_words\n\ndef calculate_tp_score(y_pred, y_true):\n    y_pred, y_true = set(y_pred), set(y_true)\n    tp = y_pred & y_true\n    # fp = y_pred - tp\n    # tn = 0.001\n    # fn = y_true - y_pred\n    # precision = len(tp) / (len(tp) + len(fp))\n    # recall = len(tp) / (len(tp) + len(fn))\n    # try:\n    #     f1_score = 2*(precision * recall)/(precision + recall)\n    # except ZeroDivisionError:\n    #     f1_score = 0\n    return len(tp)/len(y_true)\n\ndef word2vec_check_all_word_in_model(words, index2word):\n    return all([word in index2word for word in words])\n\n@ray.remote\ndef word2vec_test_association_dataset(model, model_name, dataset_name, dataset):\n    # word2vec_model = get_pinned_object(model)\n    word2vec_model = model\n    possible_top_n = [10]\n    methods = [\"IN-IN\", \"IN-OUT\", \"OUT-IN\", \"OUT-OUT\"]\n    max_possible_top_n = max(possible_top_n)\n    all_cue_test_result= []\n    for cue_name, cue_data in dataset.groupby(['cue']):\n        expected_responses = cue_data['response'].values\n        if not word2vec_check_all_word_in_model([cue_name] + list(expected_responses), word2vec_model.wv.index2word):\n            continue\n        cue_test_result = {'cue': cue_name, 'expected_responses': expected_responses}\n        for method in methods:\n            model_response = word2vec_find_top_similar_words(word2vec_model, cue_name, method, top_n=max_possible_top_n)\n            for top_n in possible_top_n:\n                subset_model_response = [x[0] for x in model_response[:top_n]]\n                tp_score = calculate_tp_score(subset_model_response, expected_responses)\n                model_method_topn_name = f'{model_name}_{method}_{top_n}'\n                cue_test_result[f'{model_method_topn_name}_model_response'] = subset_model_response\n                cue_test_result[f'{model_method_topn_name}_tp_score'] = tp_score\n        all_cue_test_result.append(cue_test_result)\n    return dataset_name, all_cue_test_result\n\n## \n# ANALGOY\n##\ndef cos3mul(compare_matrix, word_index, a, a_s, b,  b_s, top_n, e=0.0000001):\n    \"\"\"From: https://www.aclweb.org/anthology/W14-1618.pdf; Page 175\n    \"\"\"\n    c = np.stack([b, a_s, a], axis=1)\n    all_scores = np.dot(compare_matrix, c)\n    all_scores = (all_scores + 1)/2 # normalize bw 0 to 1\n    all_scores[:, 2] += e\n    all_scores = ((all_scores[:, 0] * all_scores[:, 1]) / all_scores[:, 2])\n    top_n_similar_words = np.argpartition(-all_scores, top_n)[:top_n]\n    sorted_result = sorted([(word_index[index], all_scores[index]) for index in top_n_similar_words], \n                key=lambda x: x[1], \n                reverse=True)\n    sorted_result = [x[0] for x in sorted_result]\n    return sorted_result, 1 if b_s in sorted_result else 0\n\ndef cos3add(compare_matrix, a, a_s, b, b_s, top_n, index2word):\n    \"\"\"Find the most similar vector with (b-a+a_)\n    \"\"\"\n    b_ = b.reshape(1, -1)-a.reshape(1, -1)+a_s.reshape(1, -1)\n    res = word2vec_find_top_similar_vectors_bw_matrix(\n            b_, compare_matrix, None, index2word, top_n=top_n, no_self_similarity=False)\n    res = [x[0] for x in res]\n    return res, 1 if b_s in res else 0\n\n# for dataset_name, dataset in all_datasets['analogy_datasets'].items():\n    # dataset_result = []\n@ray.remote\ndef word2vec_test_analogy_dataset(model, model_name, dataset_name, dataset):\n    dataset_result = []\n    # word2vec_model = get_pinned_object(model)\n    word2vec_model = model\n    for row in dataset:\n        a, a_s, b, b_s = row\n        try:\n            a_index = word2vec_get_index_by_word(word2vec_model, a)\n            a_s_index = word2vec_get_index_by_word(word2vec_model, a_s)\n            b_index = word2vec_get_index_by_word(word2vec_model, b)\n            b_s_index = word2vec_get_index_by_word(word2vec_model, b_s)\n        except:\n            continue\n        for method in [\"IN-IN\", \"IN-OUT\", \"OUT-IN\", \"OUT-OUT\"]:\n            source_matrix, compare_matrix = word2vec_get_model_matrix_by_method(word2vec_model, method)\n            a_vector = source_matrix[a_index]\n            a_s_vector = source_matrix[a_s_index]\n            b_vector = source_matrix[b_index]\n            cos3add_response, cos3add_score = \\\n                cos3add(compare_matrix.T, a_vector, a_s_vector, b_vector, b_s, 3, word2vec_model.wv.index2word)\n            cos3mul_response, cos3mul_score = \\\n                cos3mul(compare_matrix, word2vec_model.wv.index2word, a_vector, a_s_vector, b_vector, b_s, top_n=3)\n            dataset_result.append({\n                'analogy': row,\n                'method': method, \n                'model': model_name,\n                'cos3add_response': cos3add_response,\n                'cos3add_score': cos3add_score,\n                'cos3mul_response': cos3mul_response,\n                'cos3mul_score': cos3mul_score\n            })\n    return dataset_name, pd.DataFrame(dataset_result)","execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-d2ac0af6-f8a9-4ee2-a089-e9d4462d6e83"},"source":"# ['car', 'truck'] not in get_pinned_object(model).wv.index2word\n# all([True, False, True])\n# [5] + [1,2,3]","execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Run Analogy","metadata":{"tags":[],"cell_id":"00014-42247abb-1785-4e0b-9082-f1acadffd62a"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00015-23b99a82-7711-4d83-8d15-2ec6ee97ac91"},"source":"# perform this for each model\nfor model_dir in tqdm(glob_w2v_models[3:]):\n    model_name = model_dir.replace(\"../../../embeddings_lemma/\", \"\").replace(\"iter=5_\", \"\")\n    model_path = glob.glob(model_dir + \"/*[!(npy)]\")[0]\n    model = Word2Vec.load(model_path)\n    model.wv.vectors = nnorm(model.wv.vectors) \n    model.trainables.syn1neg = nnorm(model.trainables.syn1neg)\n    # model = pin_in_object_store(model)\n    # print(\"Running analysis on each dataset\")\n    # Step 1: Handle relatedness dataset\n    futures = [word2vec_test_analogy_dataset.remote(model, model_name, dataset_name, dataset) \\\n                    for dataset_name, dataset in all_datasets['analogy_datasets'].items()]\n    res = ray.get(futures)    \n    # print(\"Post processing and Saving results\")\n    # pd.concat(res, axis=1)\n    df_res = {dname:df_res for dname, df_res in res}\n    with open(f\"../output/word2vec/analogy/word2vec_results_{model_name}_analogy.pickle\", \"wb\") as f:\n        pickle.dump({\"score_matrix\": df_res}, f)\n    # clean ray object store\n    # ray.internal.free(model)\n    # message from telegram\n    try:\n        send_telegram_message(model_name)\n        send_telegram_message(str(df_res['google_analogy'].groupby('method').mean()))\n        send_telegram_message(str(df_res['bats_analogy'].groupby('method').mean()))\n    except:\n        pass\n                ","execution_count":null,"outputs":[{"name":"stderr","text":" 67%|██████▋   | 6/9 [2:47:33<1:28:26, 1768.90s/it]","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9/9 [4:46:47<00:00, 1911.90s/it]  \n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00016-32ac2072-d32b-4aeb-bc03-6d4eec673a12"},"source":"# glob.glob(\"../../../embeddings_lemma/word2vec_mc=10_iter=5_size=100_window=5_sg=0/*\")\nimport pickle\nwith open(\"../output/word2vec/analogy/word2vec_results_word2vec_mc=10_size=100_window=5_sg=0_analogy.pickle\", \"rb\") as f:\n    _ = pickle.load(f)\n# _['score_matrix'][0]\n(_['score_matrix']['bats_analogy'].groupby('method').mean())\n# _['score_matrix']['google_analogy'].query('method==\"IN-OUT\"')","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":4,"column_count":2,"columns":[{"name":"cos3add_score","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0,"max":0.27209302325581397,"histogram":[{"bin_start":0,"bin_end":0.027209302325581396,"count":1},{"bin_start":0.027209302325581396,"bin_end":0.05441860465116279,"count":0},{"bin_start":0.05441860465116279,"bin_end":0.08162790697674419,"count":0},{"bin_start":0.08162790697674419,"bin_end":0.10883720930232559,"count":0},{"bin_start":0.10883720930232559,"bin_end":0.13604651162790699,"count":0},{"bin_start":0.13604651162790699,"bin_end":0.16325581395348837,"count":1},{"bin_start":0.16325581395348837,"bin_end":0.19046511627906978,"count":0},{"bin_start":0.19046511627906978,"bin_end":0.21767441860465117,"count":1},{"bin_start":0.21767441860465117,"bin_end":0.24488372093023256,"count":0},{"bin_start":0.24488372093023256,"bin_end":0.27209302325581397,"count":1}]}},{"name":"cos3mul_score","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0,"max":0.2302325581395349,"histogram":[{"bin_start":0,"bin_end":0.023023255813953487,"count":1},{"bin_start":0.023023255813953487,"bin_end":0.046046511627906975,"count":0},{"bin_start":0.046046511627906975,"bin_end":0.06906976744186047,"count":0},{"bin_start":0.06906976744186047,"bin_end":0.09209302325581395,"count":0},{"bin_start":0.09209302325581395,"bin_end":0.11511627906976743,"count":0},{"bin_start":0.11511627906976743,"bin_end":0.13813953488372094,"count":0},{"bin_start":0.13813953488372094,"bin_end":0.16116279069767442,"count":1},{"bin_start":0.16116279069767442,"bin_end":0.1841860465116279,"count":0},{"bin_start":0.1841860465116279,"bin_end":0.20720930232558138,"count":1},{"bin_start":0.20720930232558138,"bin_end":0.2302325581395349,"count":1}]}},{"name":"_deepnote_index_column","dtype":"object"}],"rows_top":[{"cos3add_score":0.27209302325581397,"cos3mul_score":0.2302325581395349,"_deepnote_index_column":"IN-IN"},{"cos3add_score":0.15348837209302327,"cos3mul_score":0.1511627906976744,"_deepnote_index_column":"IN-OUT"},{"cos3add_score":0,"cos3mul_score":0,"_deepnote_index_column":"OUT-IN"},{"cos3add_score":0.2069767441860465,"cos3mul_score":0.20465116279069767,"_deepnote_index_column":"OUT-OUT"}],"rows_bottom":null},"text/plain":"         cos3add_score  cos3mul_score\nmethod                               \nIN-IN         0.272093       0.230233\nIN-OUT        0.153488       0.151163\nOUT-IN        0.000000       0.000000\nOUT-OUT       0.206977       0.204651","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cos3add_score</th>\n      <th>cos3mul_score</th>\n    </tr>\n    <tr>\n      <th>method</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>IN-IN</th>\n      <td>0.272093</td>\n      <td>0.230233</td>\n    </tr>\n    <tr>\n      <th>IN-OUT</th>\n      <td>0.153488</td>\n      <td>0.151163</td>\n    </tr>\n    <tr>\n      <th>OUT-IN</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>OUT-OUT</th>\n      <td>0.206977</td>\n      <td>0.204651</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00017-2df67f67-35a1-4951-bd8c-93266efbdec0"},"source":"","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"'listens'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Run Relatedness tests","metadata":{"tags":[],"cell_id":"00016-a624bffc-10ff-4feb-959a-457ee38599db"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00016-ff3acfd2-7377-4bad-85fa-dbc0dbe8b3f5"},"source":"# all_df_res = []\n# all_missing_words = []\n\n# perform this for each model\n# for model_dir in tqdm(glob.glob(\"../../../embeddings_lemma/word2vec_*\")[3:]):\nfor model_dir in tqdm(glob_w2v_models):\n    model_name = model_dir.replace(\"../../../embeddings_lemma/\", \"\").replace(\"iter=5_\", \"\")\n    model_path = glob.glob(model_dir + \"/*[!(npy)]\")[0]\n    model = Word2Vec.load(model_path)\n    model.wv.vectors = nnorm(model.wv.vectors) \n    model.trainables.syn1neg = nnorm(model.trainables.syn1neg)\n    # print(\"Running analysis on each dataset\")\n    # Step 1: Handle relatedness dataset\n    futures = [compare_word2vec_model_with_relatedness_dataset.remote(model, model_name, dataset_name, dataset) \\\n                    for dataset_name, dataset in all_datasets['relatedness_datasets'].items()]\n    res = ray.get(futures)    \n    # print(\"Post processing and Saving results\")\n    # pd.concat(res, axis=1)\n    df_res = pd.concat([df_res for df_res, _, _ in res], axis=1)\n    missing_words = {key:val for _, key, val in res}\n    # all_df_res.append(df_res)\n    # all_missing_words.append(missing_words)\n    with open(f\"../output/word2vec/relatedness/word2vec_results_{model_name}_relatedness.pickle\", \"wb\") as f:\n        pickle.dump({\"score_matrix\": df_res, 'missing_words': missing_words}, f)\n    try:\n        send_telegram_message(model_name)\n        send_telegram_message(str(df_res.iloc[:, [0]]))\n    except:\n        pass","execution_count":null,"outputs":[{"name":"stderr","text":"  0%|          | 0/12 [00:00<?, ?it/s]\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:52:15.305629   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008004000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n2020-09-01 19:52:19,004\tINFO (unknown file):0 -- gc.collect() freed 16 refs in 3.551877399906516 seconds\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:52:16.324450   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008004000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[36m(pid=249)\u001b[0m 2020-09-01 19:52:16,983\tINFO (unknown file):0 -- gc.collect() freed 7 refs in 1.5036049876362085 seconds\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:52:18.325723   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008004000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:52:22.326722   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008004000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:52:49.424046   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008005000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:52:50.425778   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008005000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[36m(pid=247)\u001b[0m 2020-09-01 19:52:51,213\tINFO (unknown file):0 -- gc.collect() freed 7 refs in 1.1586590576916933 seconds\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:52:52.426862   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008005000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:52:56.431663   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008005000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:53:21.806563   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008006000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:53:22.813490   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008006000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:53:24.817822   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008006000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:53:28.819188   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008006000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:53:36.826557   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008006000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0901 19:53:52.827723   218   233 store.cc:252] Not enough memory to create the object ffffffffffffffffffffffff0100008006000000, data_size=769881459, metadata_size=6, will send a reply of PlasmaError::OutOfMemory\n  0%|          | 0/12 [03:28<?, ?it/s]\n","output_type":"stream"},{"output_type":"error","ename":"ObjectStoreFullError","evalue":"Failed to put object ffffffffffffffffffffffff0100008006000000 in object store because it is full. Object size is 769881459 bytes.\nThe local object store is full of objects that are still in scope and cannot be evicted. Try increasing the object store memory available with ray.init(object_store_memory=<bytes>). You can also try setting an option to fallback to LRU eviction when the object store is full by calling ray.init(lru_evict=True). See also: https://docs.ray.io/en/latest/memory-management.html.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mObjectStoreFullError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-155-b6b34bfe9298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Step 1: Handle relatedness dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     futures = [compare_word2vec_model_with_relatedness_dataset.remote(model, model_name, dataset_name, dataset) \\\n\u001b[0;32m---> 15\u001b[0;31m                     for dataset_name, dataset in all_datasets['relatedness_datasets'].items()]\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# print(\"Post processing and Saving results\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-155-b6b34bfe9298>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Step 1: Handle relatedness dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     futures = [compare_word2vec_model_with_relatedness_dataset.remote(model, model_name, dataset_name, dataset) \\\n\u001b[0;32m---> 15\u001b[0;31m                     for dataset_name, dataset in all_datasets['relatedness_datasets'].items()]\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# print(\"Post processing and Saving results\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/ray/remote_function.py\u001b[0m in \u001b[0;36m_remote_proxy\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_remote_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_remote_proxy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/ray/remote_function.py\u001b[0m in \u001b[0;36m_remote\u001b[0;34m(self, args, kwargs, num_return_vals, is_direct_call, num_cpus, num_gpus, memory, object_store_memory, resources, max_retries)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0minvocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minvocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/ray/remote_function.py\u001b[0m in \u001b[0;36minvocation\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m             object_refs = worker.core_worker.submit_task(\n\u001b[1;32m    207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 num_return_vals, resources, max_retries)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_refs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.submit_task\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.submit_task\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.prepare_args\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.put_serialized_object\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker._create_put_buffer\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n","\u001b[0;31mObjectStoreFullError\u001b[0m: Failed to put object ffffffffffffffffffffffff0100008006000000 in object store because it is full. Object size is 769881459 bytes.\nThe local object store is full of objects that are still in scope and cannot be evicted. Try increasing the object store memory available with ray.init(object_store_memory=<bytes>). You can also try setting an option to fallback to LRU eviction when the object store is full by calling ray.init(lru_evict=True). See also: https://docs.ray.io/en/latest/memory-management.html."]}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00021-d2d51152-fe26-4a66-8e51-8ce8c479de9f"},"source":"## NO RAY version\n\n# perform this for each model\n# for model_dir in tqdm(glob.glob(\"../../../embeddings_lemma/word2vec_*\")[3:]):\nfor model_dir in tqdm(glob_w2v_models):\n    model_name = model_dir.replace(\"../../../embeddings_lemma/\", \"\").replace(\"iter=5_\", \"\")\n    model_path = glob.glob(model_dir + \"/*[!(npy)]\")[0]\n    model = Word2Vec.load(model_path)\n    model.wv.vectors = nnorm(model.wv.vectors) \n    model.trainables.syn1neg = nnorm(model.trainables.syn1neg)\n    # print(\"Running analysis on each dataset\")\n    # Step 1: Handle relatedness dataset\n    all_df_res = []\n    for dataset_name, dataset in all_datasets['relatedness_datasets'].items():\n        df_res = compare_word2vec_model_with_relatedness_dataset(model, model_name, dataset_name, dataset)\n        all_df_res.append(df_res)\n    with open(f\"../output/word2vec/relatedness/word2vec_results_{model_name}_relatedness.pickle\", \"wb\") as f:\n        pickle.dump({\"score_matrix\": all_df_res}, f)\n    try:\n        send_telegram_message(model_name)\n        send_telegram_message(str(df_res.iloc[:, [0]]))\n    except:\n        pass","execution_count":11,"outputs":[{"name":"stderr","text":"\r  0%|          | 0/12 [00:00<?, ?it/s]","output_type":"stream"}]},{"cell_type":"code","metadata":{"allow_embed":false,"cell_id":"00012-1bf4c181-882e-4c11-8704-ec97b5fa2b31","tags":[]},"source":"# glob.glob(\"../../../embeddings_lemma/word2vec_mc=10_iter=5_size=100_window=5_sg=0/*\")\nimport pickle\nwith open(\"../output/word2vec/word2vec_results_word2vec_mc=10_size=100_window=5_sg=0_relatedness.pickle\", \"rb\") as f:\n    _ = pickle.load(f)\n_['score_matrix'][0]","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":4,"column_count":13,"columns":[{"name":"EN-VERB-143","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.2045174598297288,"max":0.39746927488993555,"histogram":[{"bin_start":0.2045174598297288,"bin_end":0.22381264133574946,"count":1},{"bin_start":0.22381264133574946,"bin_end":0.24310782284177015,"count":0},{"bin_start":0.24310782284177015,"bin_end":0.26240300434779085,"count":1},{"bin_start":0.26240300434779085,"bin_end":0.2816981858538115,"count":1},{"bin_start":0.2816981858538115,"bin_end":0.3009933673598322,"count":0},{"bin_start":0.3009933673598322,"bin_end":0.32028854886585284,"count":0},{"bin_start":0.32028854886585284,"bin_end":0.3395837303718735,"count":0},{"bin_start":0.3395837303718735,"bin_end":0.3588789118778942,"count":0},{"bin_start":0.3588789118778942,"bin_end":0.37817409338391483,"count":0},{"bin_start":0.37817409338391483,"bin_end":0.39746927488993555,"count":1}]}},{"name":"EN-SimVerb-3500","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.14138012756419324,"max":0.21731962411121528,"histogram":[{"bin_start":0.14138012756419324,"bin_end":0.14897407721889544,"count":2},{"bin_start":0.14897407721889544,"bin_end":0.15656802687359764,"count":0},{"bin_start":0.15656802687359764,"bin_end":0.16416197652829984,"count":0},{"bin_start":0.16416197652829984,"bin_end":0.17175592618300206,"count":0},{"bin_start":0.17175592618300206,"bin_end":0.17934987583770426,"count":0},{"bin_start":0.17934987583770426,"bin_end":0.18694382549240646,"count":0},{"bin_start":0.18694382549240646,"bin_end":0.19453777514710865,"count":0},{"bin_start":0.19453777514710865,"bin_end":0.20213172480181085,"count":1},{"bin_start":0.20213172480181085,"bin_end":0.20972567445651308,"count":0},{"bin_start":0.20972567445651308,"bin_end":0.21731962411121528,"count":1}]}},{"name":"EN-RG-65","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.6055483366333668,"max":0.8019565031250883,"histogram":[{"bin_start":0.6055483366333668,"bin_end":0.625189153282539,"count":1},{"bin_start":0.625189153282539,"bin_end":0.6448299699317112,"count":0},{"bin_start":0.6448299699317112,"bin_end":0.6644707865808832,"count":0},{"bin_start":0.6644707865808832,"bin_end":0.6841116032300554,"count":0},{"bin_start":0.6841116032300554,"bin_end":0.7037524198792275,"count":0},{"bin_start":0.7037524198792275,"bin_end":0.7233932365283997,"count":0},{"bin_start":0.7233932365283997,"bin_end":0.7430340531775719,"count":1},{"bin_start":0.7430340531775719,"bin_end":0.762674869826744,"count":0},{"bin_start":0.762674869826744,"bin_end":0.7823156864759161,"count":0},{"bin_start":0.7823156864759161,"bin_end":0.8019565031250883,"count":2}]}},{"name":"EN-RW-STANFORD","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.15552612236157692,"max":0.414003281667331,"histogram":[{"bin_start":0.15552612236157692,"bin_end":0.18137383829215234,"count":1},{"bin_start":0.18137383829215234,"bin_end":0.20722155422272776,"count":0},{"bin_start":0.20722155422272776,"bin_end":0.23306927015330314,"count":0},{"bin_start":0.23306927015330314,"bin_end":0.2589169860838786,"count":1},{"bin_start":0.2589169860838786,"bin_end":0.284764702014454,"count":0},{"bin_start":0.284764702014454,"bin_end":0.31061241794502936,"count":0},{"bin_start":0.31061241794502936,"bin_end":0.3364601338756048,"count":0},{"bin_start":0.3364601338756048,"bin_end":0.3623078498061802,"count":0},{"bin_start":0.3623078498061802,"bin_end":0.3881555657367556,"count":0},{"bin_start":0.3881555657367556,"bin_end":0.414003281667331,"count":2}]}},{"name":"EN-MTurk-771","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.4977091531872367,"max":0.6587903550055381,"histogram":[{"bin_start":0.4977091531872367,"bin_end":0.5138172733690668,"count":1},{"bin_start":0.5138172733690668,"bin_end":0.529925393550897,"count":0},{"bin_start":0.529925393550897,"bin_end":0.5460335137327271,"count":0},{"bin_start":0.5460335137327271,"bin_end":0.5621416339145573,"count":0},{"bin_start":0.5621416339145573,"bin_end":0.5782497540963873,"count":0},{"bin_start":0.5782497540963873,"bin_end":0.5943578742782175,"count":0},{"bin_start":0.5943578742782175,"bin_end":0.6104659944600477,"count":0},{"bin_start":0.6104659944600477,"bin_end":0.6265741146418777,"count":0},{"bin_start":0.6265741146418777,"bin_end":0.6426822348237079,"count":0},{"bin_start":0.6426822348237079,"bin_end":0.6587903550055381,"count":3}]}},{"name":"EN-MEN-TR-3k","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.5902626534276089,"max":0.7667695758642483,"histogram":[{"bin_start":0.5902626534276089,"bin_end":0.6079133456712729,"count":1},{"bin_start":0.6079133456712729,"bin_end":0.6255640379149368,"count":0},{"bin_start":0.6255640379149368,"bin_end":0.6432147301586006,"count":0},{"bin_start":0.6432147301586006,"bin_end":0.6608654224022646,"count":0},{"bin_start":0.6608654224022646,"bin_end":0.6785161146459286,"count":0},{"bin_start":0.6785161146459286,"bin_end":0.6961668068895925,"count":0},{"bin_start":0.6961668068895925,"bin_end":0.7138174991332564,"count":0},{"bin_start":0.7138174991332564,"bin_end":0.7314681913769204,"count":1},{"bin_start":0.7314681913769204,"bin_end":0.7491188836205844,"count":0},{"bin_start":0.7491188836205844,"bin_end":0.7667695758642483,"count":2}]}},{"name":"EN-MC-30","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.6881511709607714,"max":0.8323022512021407,"histogram":[{"bin_start":0.6881511709607714,"bin_end":0.7025662789849083,"count":1},{"bin_start":0.7025662789849083,"bin_end":0.7169813870090452,"count":0},{"bin_start":0.7169813870090452,"bin_end":0.7313964950331822,"count":0},{"bin_start":0.7313964950331822,"bin_end":0.7458116030573191,"count":0},{"bin_start":0.7458116030573191,"bin_end":0.760226711081456,"count":0},{"bin_start":0.760226711081456,"bin_end":0.774641819105593,"count":0},{"bin_start":0.774641819105593,"bin_end":0.78905692712973,"count":1},{"bin_start":0.78905692712973,"bin_end":0.8034720351538669,"count":0},{"bin_start":0.8034720351538669,"bin_end":0.8178871431780038,"count":0},{"bin_start":0.8178871431780038,"bin_end":0.8323022512021407,"count":2}]}},{"name":"EN-MTurk-287","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.529686774390123,"max":0.7144052798698126,"histogram":[{"bin_start":0.529686774390123,"bin_end":0.548158624938092,"count":1},{"bin_start":0.548158624938092,"bin_end":0.5666304754860609,"count":0},{"bin_start":0.5666304754860609,"bin_end":0.5851023260340299,"count":0},{"bin_start":0.5851023260340299,"bin_end":0.6035741765819989,"count":0},{"bin_start":0.6035741765819989,"bin_end":0.6220460271299678,"count":0},{"bin_start":0.6220460271299678,"bin_end":0.6405178776779368,"count":0},{"bin_start":0.6405178776779368,"bin_end":0.6589897282259057,"count":1},{"bin_start":0.6589897282259057,"bin_end":0.6774615787738747,"count":0},{"bin_start":0.6774615787738747,"bin_end":0.6959334293218437,"count":1},{"bin_start":0.6959334293218437,"bin_end":0.7144052798698126,"count":1}]}},{"name":"EN-SIMLEX-999","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.26235457814126206,"max":0.3774817412189316,"histogram":[{"bin_start":0.26235457814126206,"bin_end":0.273867294449029,"count":1},{"bin_start":0.273867294449029,"bin_end":0.285380010756796,"count":1},{"bin_start":0.285380010756796,"bin_end":0.29689272706456293,"count":0},{"bin_start":0.29689272706456293,"bin_end":0.3084054433723299,"count":0},{"bin_start":0.3084054433723299,"bin_end":0.31991815968009685,"count":0},{"bin_start":0.31991815968009685,"bin_end":0.3314308759878638,"count":1},{"bin_start":0.3314308759878638,"bin_end":0.3429435922956307,"count":0},{"bin_start":0.3429435922956307,"bin_end":0.35445630860339766,"count":0},{"bin_start":0.35445630860339766,"bin_end":0.3659690249111646,"count":0},{"bin_start":0.3659690249111646,"bin_end":0.3774817412189316,"count":1}]}},{"name":"EN-WS-353-REL","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.4758090741994607,"max":0.6367034822341671,"histogram":[{"bin_start":0.4758090741994607,"bin_end":0.49189851500293136,"count":1},{"bin_start":0.49189851500293136,"bin_end":0.507987955806402,"count":0},{"bin_start":0.507987955806402,"bin_end":0.5240773966098726,"count":0},{"bin_start":0.5240773966098726,"bin_end":0.5401668374133433,"count":0},{"bin_start":0.5401668374133433,"bin_end":0.5562562782168139,"count":1},{"bin_start":0.5562562782168139,"bin_end":0.5723457190202845,"count":0},{"bin_start":0.5723457190202845,"bin_end":0.5884351598237552,"count":0},{"bin_start":0.5884351598237552,"bin_end":0.6045246006272258,"count":0},{"bin_start":0.6045246006272258,"bin_end":0.6206140414306964,"count":0},{"bin_start":0.6206140414306964,"bin_end":0.6367034822341671,"count":2}]}},{"name":"EN-YP-130","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.30044356848489134,"max":0.4632022954239011,"histogram":[{"bin_start":0.30044356848489134,"bin_end":0.3167194411787923,"count":1},{"bin_start":0.3167194411787923,"bin_end":0.3329953138726933,"count":0},{"bin_start":0.3329953138726933,"bin_end":0.3492711865665943,"count":0},{"bin_start":0.3492711865665943,"bin_end":0.36554705926049524,"count":0},{"bin_start":0.36554705926049524,"bin_end":0.3818229319543962,"count":0},{"bin_start":0.3818229319543962,"bin_end":0.3980988046482972,"count":1},{"bin_start":0.3980988046482972,"bin_end":0.4143746773421982,"count":0},{"bin_start":0.4143746773421982,"bin_end":0.4306505500360992,"count":0},{"bin_start":0.4306505500360992,"bin_end":0.44692642273000016,"count":1},{"bin_start":0.44692642273000016,"bin_end":0.4632022954239011,"count":1}]}},{"name":"EN-WS-353-ALL","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.5516145103621383,"max":0.6704490061266352,"histogram":[{"bin_start":0.5516145103621383,"bin_end":0.563497959938588,"count":1},{"bin_start":0.563497959938588,"bin_end":0.5753814095150377,"count":0},{"bin_start":0.5753814095150377,"bin_end":0.5872648590914873,"count":0},{"bin_start":0.5872648590914873,"bin_end":0.599148308667937,"count":0},{"bin_start":0.599148308667937,"bin_end":0.6110317582443867,"count":0},{"bin_start":0.6110317582443867,"bin_end":0.6229152078208364,"count":0},{"bin_start":0.6229152078208364,"bin_end":0.6347986573972861,"count":1},{"bin_start":0.6347986573972861,"bin_end":0.6466821069737357,"count":0},{"bin_start":0.6466821069737357,"bin_end":0.6585655565501854,"count":0},{"bin_start":0.6585655565501854,"bin_end":0.6704490061266352,"count":2}]}},{"name":"EN-WS-353-SIM","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":0.6045634941309105,"max":0.7297362268043539,"histogram":[{"bin_start":0.6045634941309105,"bin_end":0.6170807673982548,"count":1},{"bin_start":0.6170807673982548,"bin_end":0.6295980406655992,"count":0},{"bin_start":0.6295980406655992,"bin_end":0.6421153139329435,"count":0},{"bin_start":0.6421153139329435,"bin_end":0.6546325872002878,"count":0},{"bin_start":0.6546325872002878,"bin_end":0.6671498604676322,"count":0},{"bin_start":0.6671498604676322,"bin_end":0.6796671337349766,"count":0},{"bin_start":0.6796671337349766,"bin_end":0.6921844070023209,"count":0},{"bin_start":0.6921844070023209,"bin_end":0.7047016802696652,"count":0},{"bin_start":0.7047016802696652,"bin_end":0.7172189535370096,"count":0},{"bin_start":0.7172189535370096,"bin_end":0.7297362268043539,"count":3}]}},{"name":"_deepnote_index_column","dtype":"object"}],"rows_top":[{"EN-VERB-143":0.39746927488993555,"EN-SimVerb-3500":0.21731962411121528,"EN-RG-65":0.7339761489480742,"EN-RW-STANFORD":0.39923584983394395,"EN-MTurk-771":0.6587903550055381,"EN-MEN-TR-3k":0.7205426693458772,"EN-MC-30":0.7881115853499098,"EN-MTurk-287":0.7144052798698126,"EN-SIMLEX-999":0.3774817412189316,"EN-WS-353-REL":0.5544344817337636,"EN-YP-130":0.396154225645398,"EN-WS-353-ALL":0.6293832518117112,"EN-WS-353-SIM":0.7208313695189691,"_deepnote_index_column":"word2vec_mc=10_size=100_window=5_sg=0_IN-IN"},{"EN-VERB-143":0.2045174598297288,"EN-SimVerb-3500":0.14138012756419324,"EN-RG-65":0.8019565031250883,"EN-RW-STANFORD":0.15552612236157692,"EN-MTurk-771":0.6560076486521655,"EN-MEN-TR-3k":0.7649573922625617,"EN-MC-30":0.8323022512021407,"EN-MTurk-287":0.6577932250425418,"EN-SIMLEX-999":0.28006676307827794,"EN-WS-353-REL":0.6367034822341671,"EN-YP-130":0.44303416827663616,"EN-WS-353-ALL":0.6704490061266352,"EN-WS-353-SIM":0.7297362268043539,"_deepnote_index_column":"word2vec_mc=10_size=100_window=5_sg=0_IN-OUT"},{"EN-VERB-143":0.27155124547580356,"EN-SimVerb-3500":0.14498121166398756,"EN-RG-65":0.7986096159098365,"EN-RW-STANFORD":0.23524667907506983,"EN-MTurk-771":0.6474495518240032,"EN-MEN-TR-3k":0.7667695758642483,"EN-MC-30":0.8230561715516306,"EN-MTurk-287":0.679872611505997,"EN-SIMLEX-999":0.26235457814126206,"EN-WS-353-REL":0.6316461276242601,"EN-YP-130":0.4632022954239011,"EN-WS-353-ALL":0.6633233613157942,"EN-WS-353-SIM":0.7257652560000988,"_deepnote_index_column":"word2vec_mc=10_size=100_window=5_sg=0_OUT-IN"},{"EN-VERB-143":0.2602141346223455,"EN-SimVerb-3500":0.19665309300094058,"EN-RG-65":0.6055483366333668,"EN-RW-STANFORD":0.414003281667331,"EN-MTurk-771":0.4977091531872367,"EN-MEN-TR-3k":0.5902626534276089,"EN-MC-30":0.6881511709607714,"EN-MTurk-287":0.529686774390123,"EN-SIMLEX-999":0.3249865487373889,"EN-WS-353-REL":0.4758090741994607,"EN-YP-130":0.30044356848489134,"EN-WS-353-ALL":0.5516145103621383,"EN-WS-353-SIM":0.6045634941309105,"_deepnote_index_column":"word2vec_mc=10_size=100_window=5_sg=0_OUT-OUT"}],"rows_bottom":null},"text/plain":"                                               EN-VERB-143  EN-SimVerb-3500  \\\nword2vec_mc=10_size=100_window=5_sg=0_IN-IN       0.397469         0.217320   \nword2vec_mc=10_size=100_window=5_sg=0_IN-OUT      0.204517         0.141380   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-IN      0.271551         0.144981   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-OUT     0.260214         0.196653   \n\n                                               EN-RG-65  EN-RW-STANFORD  \\\nword2vec_mc=10_size=100_window=5_sg=0_IN-IN    0.733976        0.399236   \nword2vec_mc=10_size=100_window=5_sg=0_IN-OUT   0.801957        0.155526   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-IN   0.798610        0.235247   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-OUT  0.605548        0.414003   \n\n                                               EN-MTurk-771  EN-MEN-TR-3k  \\\nword2vec_mc=10_size=100_window=5_sg=0_IN-IN        0.658790      0.720543   \nword2vec_mc=10_size=100_window=5_sg=0_IN-OUT       0.656008      0.764957   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-IN       0.647450      0.766770   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-OUT      0.497709      0.590263   \n\n                                               EN-MC-30  EN-MTurk-287  \\\nword2vec_mc=10_size=100_window=5_sg=0_IN-IN    0.788112      0.714405   \nword2vec_mc=10_size=100_window=5_sg=0_IN-OUT   0.832302      0.657793   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-IN   0.823056      0.679873   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-OUT  0.688151      0.529687   \n\n                                               EN-SIMLEX-999  EN-WS-353-REL  \\\nword2vec_mc=10_size=100_window=5_sg=0_IN-IN         0.377482       0.554434   \nword2vec_mc=10_size=100_window=5_sg=0_IN-OUT        0.280067       0.636703   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-IN        0.262355       0.631646   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-OUT       0.324987       0.475809   \n\n                                               EN-YP-130  EN-WS-353-ALL  \\\nword2vec_mc=10_size=100_window=5_sg=0_IN-IN     0.396154       0.629383   \nword2vec_mc=10_size=100_window=5_sg=0_IN-OUT    0.443034       0.670449   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-IN    0.463202       0.663323   \nword2vec_mc=10_size=100_window=5_sg=0_OUT-OUT   0.300444       0.551615   \n\n                                               EN-WS-353-SIM  \nword2vec_mc=10_size=100_window=5_sg=0_IN-IN         0.720831  \nword2vec_mc=10_size=100_window=5_sg=0_IN-OUT        0.729736  \nword2vec_mc=10_size=100_window=5_sg=0_OUT-IN        0.725765  \nword2vec_mc=10_size=100_window=5_sg=0_OUT-OUT       0.604563  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EN-VERB-143</th>\n      <th>EN-SimVerb-3500</th>\n      <th>EN-RG-65</th>\n      <th>EN-RW-STANFORD</th>\n      <th>EN-MTurk-771</th>\n      <th>EN-MEN-TR-3k</th>\n      <th>EN-MC-30</th>\n      <th>EN-MTurk-287</th>\n      <th>EN-SIMLEX-999</th>\n      <th>EN-WS-353-REL</th>\n      <th>EN-YP-130</th>\n      <th>EN-WS-353-ALL</th>\n      <th>EN-WS-353-SIM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>word2vec_mc=10_size=100_window=5_sg=0_IN-IN</th>\n      <td>0.397469</td>\n      <td>0.217320</td>\n      <td>0.733976</td>\n      <td>0.399236</td>\n      <td>0.658790</td>\n      <td>0.720543</td>\n      <td>0.788112</td>\n      <td>0.714405</td>\n      <td>0.377482</td>\n      <td>0.554434</td>\n      <td>0.396154</td>\n      <td>0.629383</td>\n      <td>0.720831</td>\n    </tr>\n    <tr>\n      <th>word2vec_mc=10_size=100_window=5_sg=0_IN-OUT</th>\n      <td>0.204517</td>\n      <td>0.141380</td>\n      <td>0.801957</td>\n      <td>0.155526</td>\n      <td>0.656008</td>\n      <td>0.764957</td>\n      <td>0.832302</td>\n      <td>0.657793</td>\n      <td>0.280067</td>\n      <td>0.636703</td>\n      <td>0.443034</td>\n      <td>0.670449</td>\n      <td>0.729736</td>\n    </tr>\n    <tr>\n      <th>word2vec_mc=10_size=100_window=5_sg=0_OUT-IN</th>\n      <td>0.271551</td>\n      <td>0.144981</td>\n      <td>0.798610</td>\n      <td>0.235247</td>\n      <td>0.647450</td>\n      <td>0.766770</td>\n      <td>0.823056</td>\n      <td>0.679873</td>\n      <td>0.262355</td>\n      <td>0.631646</td>\n      <td>0.463202</td>\n      <td>0.663323</td>\n      <td>0.725765</td>\n    </tr>\n    <tr>\n      <th>word2vec_mc=10_size=100_window=5_sg=0_OUT-OUT</th>\n      <td>0.260214</td>\n      <td>0.196653</td>\n      <td>0.605548</td>\n      <td>0.414003</td>\n      <td>0.497709</td>\n      <td>0.590263</td>\n      <td>0.688151</td>\n      <td>0.529687</td>\n      <td>0.324987</td>\n      <td>0.475809</td>\n      <td>0.300444</td>\n      <td>0.551615</td>\n      <td>0.604563</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Run association test","metadata":{"tags":[],"cell_id":"00019-b253e8bb-1190-4447-a8c3-403f98b26655"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00020-488a8409-12ab-4206-85db-f1f34aa1982f"},"source":"all_datasets['association_datasets'].keys()","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"dict_keys(['swow8500', 'eat'])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00020-2d2f9320-fa87-41e3-9345-3e9e9bcc8941"},"source":"\n# perform this for each model\n# for model_dir in tqdm(glob.glob(\"../../../embeddings_lemma/word2vec_*\")[3:]):\nfor model_dir in tqdm(glob_w2v_models):\n    model_name = model_dir.replace(\"../../../embeddings_lemma/\", \"\").replace(\"iter=5_\", \"\")\n    model_path = glob.glob(model_dir + \"/*[!(npy)]\")[0]\n    model = Word2Vec.load(model_path)\n    model.wv.vectors = nnorm(model.wv.vectors) \n    model.trainables.syn1neg = nnorm(model.trainables.syn1neg)\n    # model = pin_in_object_store(model)\n    # print(\"Running analysis on each dataset\")\n    # Step 1: Handle relatedness dataset\n    futures = [word2vec_test_association_dataset.remote(model, model_name, dataset_name, dataset) \\\n                    for dataset_name, dataset in all_datasets['association_datasets'].items()]\n    res = ray.get(futures)    \n    # print(\"Post processing and Saving results\")\n    # pd.concat(res, axis=1)\n    df_res = {dname:df_res for dname, df_res in res}\n    with open(f\"../output/word2vec/association/word2vec_results_{model_name}_association.pickle\", \"wb\") as f:\n        pickle.dump({\"score_matrix\": df_res}, f)\n    try:\n        send_telegram_message(model_name)\n    except:\n        pass","execution_count":null,"outputs":[{"name":"stderr","text":"100%|██████████| 12/12 [11:05:23<00:00, 3326.95s/it] \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Test Analogy","metadata":{"tags":[],"cell_id":"00022-6bf910b8-a94f-49cb-8bcc-7fdbe73c0393"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00023-a3bc850d-95d1-4a23-b86c-05db42e5c44f","output_cleared":false},"source":"# all_datasets['analogy_datasets']['google_analogy'][:]\ndef cos3mul(compare_matrix, word_index, a, a_s, b,  b_s, top_n, e=0.0000001):\n    c = np.stack([b, a_s, a], axis=1)\n    all_scores = np.dot(compare_matrix, c)\n    all_scores = (all_scores + 1)/2 # normalize bw 0 to 1\n    all_scores[:, 2] += e\n    all_scores = ((all_scores[:, 0] * all_scores[:, 1]) / all_scores[:, 2])\n    top_n_similar_words = np.argpartition(-all_scores, top_n)[:top_n]\n    sorted_result = sorted([(word_index[index], all_scores[index]) for index in top_n_similar_words], \n                key=lambda x: x[1], \n                reverse=True)\n    sorted_result = [x[0] for x in sorted_result]\n    return sorted_result, 1 if b_s in sorted_result else 0\n\ndef cos3add(compare_matrix, a, a_s, b, b_s, top_n, index2word):\n    \"\"\"Find the most similar vector with (b-a+a_)\n    \"\"\"\n    b_ = b.reshape(1, -1)-a.reshape(1, -1)+a_s.reshape(1, -1)\n    res = word2vec_find_top_similar_vectors_bw_matrix(\n            b_, compare_matrix, None, index2word, top_n=top_n, no_self_similarity=False)\n    res = [x[0] for x in res]\n    return res, 1 if b_s in res else 0\n\n# for dataset_name, dataset in all_datasets['analogy_datasets'].items():\n    # dataset_result = []\n@ray.remote\ndef word2vec_test_analogy_dataset(model_name, dataset, dataset_name):\n    word2vec_model = get_pinned_object(model)\n    for row in tqdm(dataset):\n        a, a_s, b, b_s = row\n        try:\n            a_index = word2vec_get_index_by_word(word2vec_model, a)\n            a_s_index = word2vec_get_index_by_word(word2vec_model, a_s)\n            b_index = word2vec_get_index_by_word(word2vec_model, b)\n            b_s_index = word2vec_get_index_by_word(word2vec_model, b_s)\n        except:\n            continue\n        for method in [\"IN-IN\", \"IN-OUT\", \"OUT-IN\", \"OUT-OUT\"]:\n            source_matrix, compare_matrix = word2vec_get_model_matrix_by_method(word2vec_model, method)\n            a_vector = source_matrix[a_index]\n            a_s_vector = source_matrix[a_s_index]\n            b_vector = source_matrix[b_index]\n            cos3add_response, cos3add_score = \\\n                cos3add(compare_matrix.T, a_vector, a_s_vector, b_vector, b_s, 3, word2vec_model.wv.index2word)\n            cos3mul_response, cos3mul_score = \\\n                cos3mul(compare_matrix, word2vec_model.wv.index2word, a_vector, a_s_vector, b_vector, b_s, top_n=3)\n            dataset_result.append({\n                'analogy': row,\n                'method': method, \n                'cos3add_response': cos3add_response,\n                'cos3add_score': cos3add_score,\n                'cos3mul_response': cos3mul_response,\n                'cos3mul_score': cos3mul_score\n            })\n    return dataset_name, pd.DataFrame(dataset_result)\n\n# perform this for each model\nfor model_dir in tqdm(glob_w2v_models):\n    model_name = model_dir.replace(\"../../../embeddings_lemma/\", \"\").replace(\"iter=5_\", \"\")\n    model_path = glob.glob(model_dir + \"/*[!(npy)]\")[0]\n    model = Word2Vec.load(model_path)\n    model.wv.vectors = nnorm(model.wv.vectors) \n    model.trainables.syn1neg = nnorm(model.trainables.syn1neg)\n    model = pin_in_object_store(model)\n    # print(\"Running analysis on each dataset\")\n    # Step 1: Handle relatedness dataset\n    futures = [word2vec_test_association_dataset.remote(model_name, dataset_name, dataset) \\\n                    for dataset_name, dataset in all_datasets['analogy_datasets'].items()]\n    res = ray.get(futures)    \n    # print(\"Post processing and Saving results\")\n    # pd.concat(res, axis=1)\n    df_res = {dname:df_res for dname, df_res in res}\n    with open(f\"../output/word2vec_results_{model_name}_analogy.pickle\", \"wb\") as f:\n        pickle.dump({\"score_matrix\": df_res}, f)\n                ","execution_count":null,"outputs":[{"name":"stderr","text":"  3%|▎         | 27/994 [00:15<09:04,  1.78it/s]\n","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-86-5022405dacad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mcos3add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompare_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_s_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mcos3mul_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos3mul_score\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mcos3mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompare_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_s_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             dataset_result.append({\n\u001b[1;32m     46\u001b[0m                 \u001b[0;34m'analogy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-86-5022405dacad>\u001b[0m in \u001b[0;36mcos3mul\u001b[0;34m(compare_matrix, word_index, a, a_s, b, b_s, top_n, e)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcos3mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompare_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mb_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0000001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompare_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# normalize bw 0 to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mall_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00022-06d4b811-1708-4c52-83f6-50782ebd09e5"},"source":"pd.DataFrame(dataset_result)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":90,"data":{"application/vnd.deepnote.dataframe.v2+json":{"error":"Traceback (most recent call last):\n  File \"/home/jovyan/.deepnote/variable_explorer.py\", line 270, in dataframe_formatter\n    return { MIME_TYPE: describe_pd_dataframe(df) }\n  File \"/home/jovyan/.deepnote/variable_explorer_helpers.py\", line 97, in describe_pd_dataframe\n    'unique_count': column.dropna().nunique(),\n  File \"/opt/venv/lib/python3.7/site-packages/pandas/core/base.py\", line 1284, in nunique\n    uniqs = self.unique()\n  File \"/opt/venv/lib/python3.7/site-packages/pandas/core/series.py\", line 1816, in unique\n    result = super().unique()\n  File \"/opt/venv/lib/python3.7/site-packages/pandas/core/base.py\", line 1246, in unique\n    result = unique1d(values)\n  File \"/opt/venv/lib/python3.7/site-packages/pandas/core/algorithms.py\", line 382, in unique\n    uniques = table.unique(values)\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1784, in pandas._libs.hashtable.PyObjectHashTable.unique\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1731, in pandas._libs.hashtable.PyObjectHashTable._unique\nTypeError: unhashable type: 'list'\n"},"text/plain":"                                      analogy   method  \\\n0          [rome, italy, islamabad, pakistan]    IN-IN   \n1          [rome, italy, islamabad, pakistan]   IN-OUT   \n2          [rome, italy, islamabad, pakistan]   OUT-IN   \n3          [rome, italy, islamabad, pakistan]  OUT-OUT   \n4       [hanoi, vietnam, canberra, australia]    IN-IN   \n..                                        ...      ...   \n105  [bangkok, thailand, canberra, australia]   IN-OUT   \n106  [bangkok, thailand, canberra, australia]   OUT-IN   \n107  [bangkok, thailand, canberra, australia]  OUT-OUT   \n108           [madrid, spain, athens, greece]    IN-IN   \n109           [madrid, spain, athens, greece]   IN-OUT   \n\n                           cos3add_response  cos3add_score  \\\n0     [islamabad, malaysia, ===pakistan===]              0   \n1                [pakistan, malaysia, iran]              1   \n2       [aa/semitic, petrochimi, tejan-sie]              0   \n3           [islamabad, pakistan, sargodha]              1   \n4        [canberra, queensland, australian]              0   \n..                                      ...            ...   \n105  [australia, canberra, ===australia===]              1   \n106           [aa/semitic, hmts, forceably]              0   \n107        [canberra, queensland, tasmania]              0   \n108          [greece, athens, peloponnesus]              1   \n109             [greece, athens, macedonia]              1   \n\n                                 cos3mul_response  cos3mul_score  \n0    [===pakistan===, ====pakistan====, malaysia]              0  \n1                [pakistan, malaysia, bangladesh]              1  \n2             [aa/semitic, petrochimi, tejan-sie]              0  \n3         [pakistan, islamabad, gilgit-baltistan]              1  \n4                   [queensland, australian, nsw]              0  \n..                                            ...            ...  \n105        [australia, ===australia===, canberra]              1  \n106                 [aa/semitic, hmts, forceably]              0  \n107              [canberra, queensland, tasmania]              0  \n108                     [greece, colchis, athens]              1  \n109                   [greece, athens, macedonia]              1  \n\n[110 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>analogy</th>\n      <th>method</th>\n      <th>cos3add_response</th>\n      <th>cos3add_score</th>\n      <th>cos3mul_response</th>\n      <th>cos3mul_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[rome, italy, islamabad, pakistan]</td>\n      <td>IN-IN</td>\n      <td>[islamabad, malaysia, ===pakistan===]</td>\n      <td>0</td>\n      <td>[===pakistan===, ====pakistan====, malaysia]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[rome, italy, islamabad, pakistan]</td>\n      <td>IN-OUT</td>\n      <td>[pakistan, malaysia, iran]</td>\n      <td>1</td>\n      <td>[pakistan, malaysia, bangladesh]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[rome, italy, islamabad, pakistan]</td>\n      <td>OUT-IN</td>\n      <td>[aa/semitic, petrochimi, tejan-sie]</td>\n      <td>0</td>\n      <td>[aa/semitic, petrochimi, tejan-sie]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[rome, italy, islamabad, pakistan]</td>\n      <td>OUT-OUT</td>\n      <td>[islamabad, pakistan, sargodha]</td>\n      <td>1</td>\n      <td>[pakistan, islamabad, gilgit-baltistan]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[hanoi, vietnam, canberra, australia]</td>\n      <td>IN-IN</td>\n      <td>[canberra, queensland, australian]</td>\n      <td>0</td>\n      <td>[queensland, australian, nsw]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>[bangkok, thailand, canberra, australia]</td>\n      <td>IN-OUT</td>\n      <td>[australia, canberra, ===australia===]</td>\n      <td>1</td>\n      <td>[australia, ===australia===, canberra]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>[bangkok, thailand, canberra, australia]</td>\n      <td>OUT-IN</td>\n      <td>[aa/semitic, hmts, forceably]</td>\n      <td>0</td>\n      <td>[aa/semitic, hmts, forceably]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>[bangkok, thailand, canberra, australia]</td>\n      <td>OUT-OUT</td>\n      <td>[canberra, queensland, tasmania]</td>\n      <td>0</td>\n      <td>[canberra, queensland, tasmania]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>[madrid, spain, athens, greece]</td>\n      <td>IN-IN</td>\n      <td>[greece, athens, peloponnesus]</td>\n      <td>1</td>\n      <td>[greece, colchis, athens]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>[madrid, spain, athens, greece]</td>\n      <td>IN-OUT</td>\n      <td>[greece, athens, macedonia]</td>\n      <td>1</td>\n      <td>[greece, athens, macedonia]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>110 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00021-ec290a1b-0c60-48c4-a75c-c40ce883b152"},"source":"# # all_datasets['analogy_datasets']['google_analogy']['gram8-plural'][:10]\n# # all_datasets['analogy_datasets']['bats_analogy'][:10]\n# # pd.DataFrame(dataset_result)\n# def cos3mul(compare_matrix, word_index, c,  b_s, top_n, e=0.0000001):\n#     all_scores = np.dot(compare_matrix, c)\n#     all_scores = (all_scores + 1)/2 # normalize bw 0 to 1\n#     all_scores[:, 2] += e\n#     all_scores = ((all_scores[:, 0] * all_scores[:, 1]) / all_scores[:, 2])\n#     top_n_similar_words = np.argpartition(-all_scores, top_n)[:top_n]\n#     sorted_result = sorted([(word_index[index], all_scores[index]) for index in top_n_similar_words], \n#                 key=lambda x: x[1], \n#                 reverse=True)\n#     sorted_result = [x[0] for x in sorted_result]\n#     return sorted_result, 1 if b_s in sorted_result else 0\n# # test\n# # source_matrix, compare_matrix = word2vec_get_model_matrix_by_method(model, \"IN-IN\")\n# # c = np.stack(\n# #     [source_matrix[word2vec_get_index_by_word(model, 'tehran')],\n# #     source_matrix[word2vec_get_index_by_word(model, 'italy')],\n# #     source_matrix[word2vec_get_index_by_word(model, 'rome')]]\n# #         , axis=1)\n# # cos3mul(compare_matrix, model.wv.index2word, c,  'pakistan', 10, e=0.0001)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":80,"data":{"text/plain":"(['iran',\n  'tehran',\n  '===iran===',\n  'azerbaijan',\n  'khodro',\n  'utc+03:30',\n  'kuwait',\n  'abadan',\n  'uae',\n  'bahrain'],\n 0)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00023-9e2b03d5-cef1-4d3e-8cf6-c895c5fdf383"},"source":"# np.stack([[1, 0], [1, 1], [0, 1]], axis=1)\n# all_dataset\n# model.wv.most_similar(positive=['italy', 'islamabad'], negative=['rome'])\n# ais = np.dot(compare_matrix, c)\n# max(((ais[:, 0] * ais[:, 1])/(ais[:, 2]+0.001)))#[1951]\nais.min()","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":78,"data":{"text/plain":"-0.4604089"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00024-b12b018f-19da-4818-839a-88fe668e041f"},"source":"# print(model.wv.index2word.index('pakistan'))\nall_datasets['analogy_datasets']['google_analogy'][:10]","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":57,"data":{"text/plain":"[['rome', 'italy', 'islamabad', 'pakistan'],\n ['hanoi', 'vietnam', 'canberra', 'australia'],\n ['ottawa', 'canada', 'havana', 'cuba'],\n ['stockholm', 'sweden', 'london', 'england'],\n ['havana', 'cuba', 'berlin', 'germany'],\n ['athens', 'greece', 'tehran', 'iran'],\n ['cairo', 'egypt', 'canberra', 'australia'],\n ['tokyo', 'japan', 'helsinki', 'finland'],\n ['islamabad', 'pakistan', 'hanoi', 'vietnam'],\n ['islamabad', 'pakistan', 'paris', 'france']]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Combine all results","metadata":{"tags":[],"cell_id":"00013-224f56bd-8280-4ed9-ab56-c489d06e58e1"}},{"cell_type":"markdown","source":"### Relatedness","metadata":{"tags":[],"cell_id":"00032-57696227-b762-49f2-8207-d99eb9d5781a"}},{"cell_type":"code","metadata":{"cell_id":"00018-0c99c113-8491-4b9d-b22b-aec61548ecff","tags":[]},"source":"# Load all relatedness results\nall_relatedness_table = []\nfor result_file in glob.glob(\"../output/word2vec/relatedness/*\"):\n    with open (result_file, \"rb\") as f:\n        data = pickle.load(f)\n        all_relatedness_table.append(data['score_matrix'][0])    \n        # print(result_file)\nall_relatedness_table = pd.concat(all_relatedness_table)\n# all_relatedness_table.to_csv(\"../output/all_relatedness_table.csv\")","execution_count":null,"outputs":[{"name":"stdout","text":"../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=200_window=50_sg=1_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=300_window=5_sg=1_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=200_window=50_sg=0_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=300_window=50_sg=0_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=200_window=5_sg=1_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=200_window=5_sg=0_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=100_window=5_sg=1_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=100_window=5_sg=0_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=300_window=50_sg=1_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=100_window=50_sg=0_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=300_window=5_sg=0_relatedness.pickle\n../output/word2vec/relatedness/word2vec_results_word2vec_mc=10_size=100_window=50_sg=1_relatedness.pickle\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00033-16e5564e-fe45-46ea-a2d1-fafbb629dcad"},"source":"# all_relatedness_table.index\n# np.allclose(all_relatedness_table[3].values, all_relatedness_table[10].values)\n# pd.concat(all_relatedness_table)\n# all_relatedness_table[10].index","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"Index(['word2vec_mc=10_size=100_window=5_sg=0_IN-IN',\n       'word2vec_mc=10_size=100_window=5_sg=0_IN-OUT',\n       'word2vec_mc=10_size=100_window=5_sg=0_OUT-IN',\n       'word2vec_mc=10_size=100_window=5_sg=0_OUT-OUT'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00035-eb722f10-fdc5-486f-91d6-c2049840470f"},"source":"","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"dict_keys(['score_matrix'])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Association","metadata":{"tags":[],"cell_id":"00036-06650a90-42a4-4128-bb47-aac9a6d8bf34"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00036-71254e37-732e-4fc1-9f2b-71d99ed9fa48"},"source":"# all association result\ndef clean_association_model_name(name): return name.replace(\"mc=10_\", \"\").replace(\"_10_tp_score\", \"\")\nall_association_result = []\nfor result_file in tqdm(glob.glob(\"../output/word2vec/association/*\")):\n    with open(result_file, \"rb\") as f:\n        data = pickle.load(f)\n        sw_data = pd.DataFrame(data['score_matrix']['swow8500'])\n        eat_data = pd.DataFrame(data['score_matrix']['eat'])\n        # x.\n        association_result = []\n        for col in sw_data.describe().columns:\n            result = {}\n            for dataset_name, dataset in [(\"swow8500\", sw_data), (\"eat\", eat_data)]:\n                def get_stats(dataset_name, dataset):\n                    hit_rate = dataset[dataset[col]>0].shape[0] / dataset.shape[0] \n                    avg_coverage =  dataset.loc[dataset[col]>0, col].mean()\n                    return {'col': clean_association_model_name(col), f'{dataset_name}_hit_rate': hit_rate, f'{dataset_name}_avg_coverage': avg_coverage}\n                result = {**result, **get_stats(dataset_name, dataset)}\n            association_result.append(result)\n        association_result = pd.DataFrame(association_result)\n        association_result.index = association_result['col']\n        association_result.drop(columns=['col'], inplace=True)\n        all_association_result.append(association_result)\nall_association_result = pd.concat(all_association_result)\nall_association_result.to_csv(\"../output/all_association_result.csv\")","execution_count":null,"outputs":[{"name":"stderr","text":"100%|██████████| 12/12 [00:17<00:00,  1.45s/it]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00038-db707de3-e638-44bc-9885-8c023ab1ac8c"},"source":"# with open('../output/word2vec/association/word2vec_results_word2vec_mc=10_size=100_window=50_sg=0_association.pickle', 'rb') as f:\n#     data = pickle.load(f)\n#     sw_data = pd.DataFrame(data['score_matrix']['swow8500'])\n#     display(sw_data.describe())\n# # glob.glob(\"../output/word2vec/association/*\")","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00039-d5b842f4-a177-46de-b207-150820ad1970"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analogy","metadata":{"tags":[],"cell_id":"00040-8da342e7-4abd-48ad-bbb7-7ab46ecde5d6"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00041-94fc5e4a-afa6-45d4-88e7-6fb6c76d5237"},"source":"# all association result\n# def clean_association_model_name(name): return name.replace(\"mc=10_\", \"\").replace(\"_10_tp_score\", \"\")\nall_analogy_result = []\nfor result_file in tqdm(glob.glob(\"../output/word2vec/analogy/*\")):\n    with open(result_file, \"rb\") as f:\n        data = pickle.load(f)\n        model_name = re.findall( \"_word2vec.*\",result_file)[0].replace(\"_analogy.pickle\", \"\")\n        def extract_result(dataset_name, model_name):\n            dataset = pd.DataFrame(data['score_matrix'][dataset_name])\n            dataset = pd.DataFrame(dataset).groupby('method').mean()\n            dataset.columns = [dataset_name + \"_\" + str(x) for x in dataset.columns]\n            dataset.index = [model_name + \"_\" + x for x in dataset.index]\n            return dataset\n        result = pd.concat([extract_result('google_analogy', model_name), \n                              extract_result('bats_analogy', model_name)], axis=1)\n        all_analogy_result.append(result)\nall_analogy_result = pd.concat(all_analogy_result)\nall_analogy_result.to_csv(\"../output/all_analogy_result.csv\")","execution_count":null,"outputs":[{"name":"stderr","text":"100%|██████████| 12/12 [00:02<00:00,  5.79it/s]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00042-c650f31e-9ca4-417b-9aa5-eaceaacf0e4d"},"source":"","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":145,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":48,"column_count":4,"columns":[{"name":"google_analogy_cos3add_score","dtype":"float64","stats":{"unique_count":41,"nan_count":0,"min":0,"max":0.7404426559356136,"histogram":[{"bin_start":0,"bin_end":0.07404426559356136,"count":6},{"bin_start":0.07404426559356136,"bin_end":0.14808853118712273,"count":0},{"bin_start":0.14808853118712273,"bin_end":0.2221327967806841,"count":0},{"bin_start":0.2221327967806841,"bin_end":0.29617706237424546,"count":1},{"bin_start":0.29617706237424546,"bin_end":0.3702213279678068,"count":0},{"bin_start":0.3702213279678068,"bin_end":0.4442655935613682,"count":4},{"bin_start":0.4442655935613682,"bin_end":0.5183098591549296,"count":3},{"bin_start":0.5183098591549296,"bin_end":0.5923541247484909,"count":12},{"bin_start":0.5923541247484909,"bin_end":0.6663983903420523,"count":12},{"bin_start":0.6663983903420523,"bin_end":0.7404426559356136,"count":10}]}},{"name":"google_analogy_cos3mul_score","dtype":"float64","stats":{"unique_count":44,"nan_count":0,"min":0,"max":0.7595573440643864,"histogram":[{"bin_start":0,"bin_end":0.07595573440643863,"count":6},{"bin_start":0.07595573440643863,"bin_end":0.15191146881287726,"count":0},{"bin_start":0.15191146881287726,"bin_end":0.2278672032193159,"count":0},{"bin_start":0.2278672032193159,"bin_end":0.3038229376257545,"count":1},{"bin_start":0.3038229376257545,"bin_end":0.3797786720321932,"count":2},{"bin_start":0.3797786720321932,"bin_end":0.4557344064386318,"count":2},{"bin_start":0.4557344064386318,"bin_end":0.5316901408450704,"count":8},{"bin_start":0.5316901408450704,"bin_end":0.607645875251509,"count":10},{"bin_start":0.607645875251509,"bin_end":0.6836016096579477,"count":11},{"bin_start":0.6836016096579477,"bin_end":0.7595573440643864,"count":8}]}},{"name":"bats_analogy_cos3add_score","dtype":"float64","stats":{"unique_count":33,"nan_count":0,"min":0,"max":0.2813953488372093,"histogram":[{"bin_start":0,"bin_end":0.02813953488372093,"count":6},{"bin_start":0.02813953488372093,"bin_end":0.05627906976744186,"count":0},{"bin_start":0.05627906976744186,"bin_end":0.0844186046511628,"count":1},{"bin_start":0.0844186046511628,"bin_end":0.11255813953488372,"count":0},{"bin_start":0.11255813953488372,"bin_end":0.14069767441860465,"count":4},{"bin_start":0.14069767441860465,"bin_end":0.1688372093023256,"count":7},{"bin_start":0.1688372093023256,"bin_end":0.19697674418604652,"count":7},{"bin_start":0.19697674418604652,"bin_end":0.22511627906976744,"count":11},{"bin_start":0.22511627906976744,"bin_end":0.2532558139534884,"count":5},{"bin_start":0.2532558139534884,"bin_end":0.2813953488372093,"count":7}]}},{"name":"bats_analogy_cos3mul_score","dtype":"float64","stats":{"unique_count":35,"nan_count":0,"min":0,"max":0.29767441860465116,"histogram":[{"bin_start":0,"bin_end":0.029767441860465114,"count":6},{"bin_start":0.029767441860465114,"bin_end":0.05953488372093023,"count":1},{"bin_start":0.05953488372093023,"bin_end":0.08930232558139534,"count":0},{"bin_start":0.08930232558139534,"bin_end":0.11906976744186046,"count":2},{"bin_start":0.11906976744186046,"bin_end":0.14883720930232558,"count":4},{"bin_start":0.14883720930232558,"bin_end":0.17860465116279067,"count":7},{"bin_start":0.17860465116279067,"bin_end":0.2083720930232558,"count":10},{"bin_start":0.2083720930232558,"bin_end":0.23813953488372092,"count":10},{"bin_start":0.23813953488372092,"bin_end":0.267906976744186,"count":5},{"bin_start":0.267906976744186,"bin_end":0.29767441860465116,"count":3}]}},{"name":"_deepnote_index_column","dtype":"object"}],"rows_top":[{"google_analogy_cos3add_score":0.6670020120724346,"google_analogy_cos3mul_score":0.6167002012072434,"bats_analogy_cos3add_score":0.2558139534883721,"bats_analogy_cos3mul_score":0.2,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=50_sg=0_IN-IN"},{"google_analogy_cos3add_score":0.6207243460764588,"google_analogy_cos3mul_score":0.6106639839034205,"bats_analogy_cos3add_score":0.19534883720930232,"bats_analogy_cos3mul_score":0.18604651162790697,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=50_sg=0_IN-OUT"},{"google_analogy_cos3add_score":0,"google_analogy_cos3mul_score":0,"bats_analogy_cos3add_score":0,"bats_analogy_cos3mul_score":0,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=50_sg=0_OUT-IN"},{"google_analogy_cos3add_score":0.5462776659959758,"google_analogy_cos3mul_score":0.5462776659959758,"bats_analogy_cos3add_score":0.23255813953488372,"bats_analogy_cos3mul_score":0.23488372093023255,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=50_sg=0_OUT-OUT"},{"google_analogy_cos3add_score":0.6468812877263581,"google_analogy_cos3mul_score":0.635814889336016,"bats_analogy_cos3add_score":0.21627906976744185,"bats_analogy_cos3mul_score":0.2116279069767442,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=5_sg=1_IN-IN"},{"google_analogy_cos3add_score":0.5372233400402414,"google_analogy_cos3mul_score":0.5050301810865191,"bats_analogy_cos3add_score":0.17209302325581396,"bats_analogy_cos3mul_score":0.16046511627906976,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=5_sg=1_IN-OUT"},{"google_analogy_cos3add_score":0.44064386317907445,"google_analogy_cos3mul_score":0.3772635814889336,"bats_analogy_cos3add_score":0.1558139534883721,"bats_analogy_cos3mul_score":0.12325581395348838,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=5_sg=1_OUT-IN"},{"google_analogy_cos3add_score":0.6086519114688129,"google_analogy_cos3mul_score":0.5935613682092555,"bats_analogy_cos3add_score":0.22325581395348837,"bats_analogy_cos3mul_score":0.21395348837209302,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=5_sg=1_OUT-OUT"},{"google_analogy_cos3add_score":0.7323943661971831,"google_analogy_cos3mul_score":0.7585513078470825,"bats_analogy_cos3add_score":0.24651162790697675,"bats_analogy_cos3mul_score":0.26744186046511625,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=5_sg=1_IN-IN"},{"google_analogy_cos3add_score":0.6287726358148893,"google_analogy_cos3mul_score":0.6267605633802817,"bats_analogy_cos3add_score":0.2116279069767442,"bats_analogy_cos3mul_score":0.20930232558139536,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=5_sg=1_IN-OUT"},{"google_analogy_cos3add_score":0.5291750503018109,"google_analogy_cos3mul_score":0.5201207243460765,"bats_analogy_cos3add_score":0.18604651162790697,"bats_analogy_cos3mul_score":0.17906976744186046,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=5_sg=1_OUT-IN"},{"google_analogy_cos3add_score":0.7032193158953722,"google_analogy_cos3mul_score":0.7334004024144869,"bats_analogy_cos3add_score":0.2558139534883721,"bats_analogy_cos3mul_score":0.2837209302325581,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=5_sg=1_OUT-OUT"},{"google_analogy_cos3add_score":0.7012072434607646,"google_analogy_cos3mul_score":0.6670020120724346,"bats_analogy_cos3add_score":0.27209302325581397,"bats_analogy_cos3mul_score":0.2302325581395349,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=5_sg=0_IN-IN"},{"google_analogy_cos3add_score":0.5201207243460765,"google_analogy_cos3mul_score":0.5150905432595574,"bats_analogy_cos3add_score":0.15348837209302327,"bats_analogy_cos3mul_score":0.1511627906976744,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=5_sg=0_IN-OUT"},{"google_analogy_cos3add_score":0,"google_analogy_cos3mul_score":0,"bats_analogy_cos3add_score":0,"bats_analogy_cos3mul_score":0,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=5_sg=0_OUT-IN"},{"google_analogy_cos3add_score":0.5905432595573441,"google_analogy_cos3mul_score":0.5895372233400402,"bats_analogy_cos3add_score":0.2069767441860465,"bats_analogy_cos3mul_score":0.20465116279069767,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=5_sg=0_OUT-OUT"},{"google_analogy_cos3add_score":0.5744466800804829,"google_analogy_cos3mul_score":0.5744466800804829,"bats_analogy_cos3add_score":0.20465116279069767,"bats_analogy_cos3mul_score":0.19069767441860466,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=50_sg=1_IN-IN"},{"google_analogy_cos3add_score":0.4879275653923541,"google_analogy_cos3mul_score":0.4798792756539235,"bats_analogy_cos3add_score":0.13023255813953488,"bats_analogy_cos3mul_score":0.13488372093023257,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=50_sg=1_IN-OUT"},{"google_analogy_cos3add_score":0.3873239436619718,"google_analogy_cos3mul_score":0.36217303822937624,"bats_analogy_cos3add_score":0.1186046511627907,"bats_analogy_cos3mul_score":0.10465116279069768,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=50_sg=1_OUT-IN"},{"google_analogy_cos3add_score":0.5593561368209256,"google_analogy_cos3mul_score":0.5774647887323944,"bats_analogy_cos3add_score":0.19069767441860466,"bats_analogy_cos3mul_score":0.1883720930232558,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=50_sg=1_OUT-OUT"},{"google_analogy_cos3add_score":0.613682092555332,"google_analogy_cos3mul_score":0.6156941649899397,"bats_analogy_cos3add_score":0.2116279069767442,"bats_analogy_cos3mul_score":0.2116279069767442,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=50_sg=1_IN-IN"},{"google_analogy_cos3add_score":0.5171026156941649,"google_analogy_cos3mul_score":0.5181086519114688,"bats_analogy_cos3add_score":0.16744186046511628,"bats_analogy_cos3mul_score":0.16046511627906976,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=50_sg=1_IN-OUT"},{"google_analogy_cos3add_score":0.4275653923541248,"google_analogy_cos3mul_score":0.4044265593561368,"bats_analogy_cos3add_score":0.1372093023255814,"bats_analogy_cos3mul_score":0.12558139534883722,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=50_sg=1_OUT-IN"},{"google_analogy_cos3add_score":0.5895372233400402,"google_analogy_cos3mul_score":0.5975855130784709,"bats_analogy_cos3add_score":0.2116279069767442,"bats_analogy_cos3mul_score":0.20232558139534884,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=50_sg=1_OUT-OUT"},{"google_analogy_cos3add_score":0.7082494969818913,"google_analogy_cos3mul_score":0.7223340040241448,"bats_analogy_cos3add_score":0.26744186046511625,"bats_analogy_cos3mul_score":0.26744186046511625,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=50_sg=0_IN-IN"},{"google_analogy_cos3add_score":0.6639839034205232,"google_analogy_cos3mul_score":0.6750503018108652,"bats_analogy_cos3add_score":0.22790697674418606,"bats_analogy_cos3mul_score":0.23255813953488372,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=50_sg=0_IN-OUT"},{"google_analogy_cos3add_score":0,"google_analogy_cos3mul_score":0,"bats_analogy_cos3add_score":0,"bats_analogy_cos3mul_score":0,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=50_sg=0_OUT-IN"},{"google_analogy_cos3add_score":0.6066398390342053,"google_analogy_cos3mul_score":0.6126760563380281,"bats_analogy_cos3add_score":0.20232558139534884,"bats_analogy_cos3mul_score":0.20930232558139536,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=50_sg=0_OUT-OUT"},{"google_analogy_cos3add_score":0.693158953722334,"google_analogy_cos3mul_score":0.6911468812877264,"bats_analogy_cos3add_score":0.27209302325581397,"bats_analogy_cos3mul_score":0.27209302325581397,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=50_sg=0_IN-IN"},{"google_analogy_cos3add_score":0.6649899396378269,"google_analogy_cos3mul_score":0.6690140845070423,"bats_analogy_cos3add_score":0.22093023255813954,"bats_analogy_cos3mul_score":0.2255813953488372,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=50_sg=0_IN-OUT"},{"google_analogy_cos3add_score":0,"google_analogy_cos3mul_score":0,"bats_analogy_cos3add_score":0,"bats_analogy_cos3mul_score":0,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=50_sg=0_OUT-IN"},{"google_analogy_cos3add_score":0.607645875251509,"google_analogy_cos3mul_score":0.6066398390342053,"bats_analogy_cos3add_score":0.21627906976744185,"bats_analogy_cos3mul_score":0.21395348837209302,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=50_sg=0_OUT-OUT"},{"google_analogy_cos3add_score":0.7374245472837022,"google_analogy_cos3mul_score":0.7434607645875252,"bats_analogy_cos3add_score":0.27674418604651163,"bats_analogy_cos3mul_score":0.2651162790697674,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=5_sg=0_IN-IN"},{"google_analogy_cos3add_score":0.5593561368209256,"google_analogy_cos3mul_score":0.5603621730382293,"bats_analogy_cos3add_score":0.14418604651162792,"bats_analogy_cos3mul_score":0.14883720930232558,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=5_sg=0_IN-OUT"},{"google_analogy_cos3add_score":0,"google_analogy_cos3mul_score":0,"bats_analogy_cos3add_score":0,"bats_analogy_cos3mul_score":0,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=5_sg=0_OUT-IN"},{"google_analogy_cos3add_score":0.6217303822937625,"google_analogy_cos3mul_score":0.6277665995975855,"bats_analogy_cos3add_score":0.1883720930232558,"bats_analogy_cos3mul_score":0.1930232558139535,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=5_sg=0_OUT-OUT"},{"google_analogy_cos3add_score":0.7112676056338029,"google_analogy_cos3mul_score":0.7183098591549296,"bats_analogy_cos3add_score":0.25116279069767444,"bats_analogy_cos3mul_score":0.2651162790697674,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=5_sg=1_IN-IN"},{"google_analogy_cos3add_score":0.6066398390342053,"google_analogy_cos3mul_score":0.60261569416499,"bats_analogy_cos3add_score":0.2069767441860465,"bats_analogy_cos3mul_score":0.19069767441860466,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=5_sg=1_IN-OUT"},{"google_analogy_cos3add_score":0.5191146881287726,"google_analogy_cos3mul_score":0.4909456740442656,"bats_analogy_cos3add_score":0.17209302325581396,"bats_analogy_cos3mul_score":0.16279069767441862,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=5_sg=1_OUT-IN"},{"google_analogy_cos3add_score":0.6800804828973843,"google_analogy_cos3mul_score":0.6921529175050302,"bats_analogy_cos3add_score":0.24883720930232558,"bats_analogy_cos3mul_score":0.24186046511627907,"_deepnote_index_column":"_word2vec_mc=10_size=200_window=5_sg=1_OUT-OUT"},{"google_analogy_cos3add_score":0.7404426559356136,"google_analogy_cos3mul_score":0.7595573440643864,"bats_analogy_cos3add_score":0.2813953488372093,"bats_analogy_cos3mul_score":0.29767441860465116,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=5_sg=0_IN-IN"},{"google_analogy_cos3add_score":0.5503018108651911,"google_analogy_cos3mul_score":0.5613682092555332,"bats_analogy_cos3add_score":0.16744186046511628,"bats_analogy_cos3mul_score":0.16744186046511628,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=5_sg=0_IN-OUT"},{"google_analogy_cos3add_score":0.001006036217303823,"google_analogy_cos3mul_score":0.014084507042253521,"bats_analogy_cos3add_score":0,"bats_analogy_cos3mul_score":0.009302325581395349,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=5_sg=0_OUT-IN"},{"google_analogy_cos3add_score":0.6066398390342053,"google_analogy_cos3mul_score":0.6207243460764588,"bats_analogy_cos3add_score":0.18372093023255814,"bats_analogy_cos3mul_score":0.1883720930232558,"_deepnote_index_column":"_word2vec_mc=10_size=300_window=5_sg=0_OUT-OUT"},{"google_analogy_cos3add_score":0.5221327967806841,"google_analogy_cos3mul_score":0.5040241448692153,"bats_analogy_cos3add_score":0.16744186046511628,"bats_analogy_cos3mul_score":0.14186046511627906,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=50_sg=1_IN-IN"},{"google_analogy_cos3add_score":0.4124748490945674,"google_analogy_cos3mul_score":0.3993963782696177,"bats_analogy_cos3add_score":0.1186046511627907,"bats_analogy_cos3mul_score":0.10232558139534884,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=50_sg=1_IN-OUT"},{"google_analogy_cos3add_score":0.2796780684104628,"google_analogy_cos3mul_score":0.23340040241448692,"bats_analogy_cos3add_score":0.06046511627906977,"bats_analogy_cos3mul_score":0.046511627906976744,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=50_sg=1_OUT-IN"},{"google_analogy_cos3add_score":0.5080482897384306,"google_analogy_cos3mul_score":0.4959758551307847,"bats_analogy_cos3add_score":0.16279069767441862,"bats_analogy_cos3mul_score":0.1511627906976744,"_deepnote_index_column":"_word2vec_mc=10_size=100_window=50_sg=1_OUT-OUT"}],"rows_bottom":null},"text/plain":"                                                 google_analogy_cos3add_score  \\\n_word2vec_mc=10_size=100_window=50_sg=0_IN-IN                        0.667002   \n_word2vec_mc=10_size=100_window=50_sg=0_IN-OUT                       0.620724   \n_word2vec_mc=10_size=100_window=50_sg=0_OUT-IN                       0.000000   \n_word2vec_mc=10_size=100_window=50_sg=0_OUT-OUT                      0.546278   \n_word2vec_mc=10_size=100_window=5_sg=1_IN-IN                         0.646881   \n_word2vec_mc=10_size=100_window=5_sg=1_IN-OUT                        0.537223   \n_word2vec_mc=10_size=100_window=5_sg=1_OUT-IN                        0.440644   \n_word2vec_mc=10_size=100_window=5_sg=1_OUT-OUT                       0.608652   \n_word2vec_mc=10_size=300_window=5_sg=1_IN-IN                         0.732394   \n_word2vec_mc=10_size=300_window=5_sg=1_IN-OUT                        0.628773   \n_word2vec_mc=10_size=300_window=5_sg=1_OUT-IN                        0.529175   \n_word2vec_mc=10_size=300_window=5_sg=1_OUT-OUT                       0.703219   \n_word2vec_mc=10_size=100_window=5_sg=0_IN-IN                         0.701207   \n_word2vec_mc=10_size=100_window=5_sg=0_IN-OUT                        0.520121   \n_word2vec_mc=10_size=100_window=5_sg=0_OUT-IN                        0.000000   \n_word2vec_mc=10_size=100_window=5_sg=0_OUT-OUT                       0.590543   \n_word2vec_mc=10_size=200_window=50_sg=1_IN-IN                        0.574447   \n_word2vec_mc=10_size=200_window=50_sg=1_IN-OUT                       0.487928   \n_word2vec_mc=10_size=200_window=50_sg=1_OUT-IN                       0.387324   \n_word2vec_mc=10_size=200_window=50_sg=1_OUT-OUT                      0.559356   \n_word2vec_mc=10_size=300_window=50_sg=1_IN-IN                        0.613682   \n_word2vec_mc=10_size=300_window=50_sg=1_IN-OUT                       0.517103   \n_word2vec_mc=10_size=300_window=50_sg=1_OUT-IN                       0.427565   \n_word2vec_mc=10_size=300_window=50_sg=1_OUT-OUT                      0.589537   \n_word2vec_mc=10_size=300_window=50_sg=0_IN-IN                        0.708249   \n_word2vec_mc=10_size=300_window=50_sg=0_IN-OUT                       0.663984   \n_word2vec_mc=10_size=300_window=50_sg=0_OUT-IN                       0.000000   \n_word2vec_mc=10_size=300_window=50_sg=0_OUT-OUT                      0.606640   \n_word2vec_mc=10_size=200_window=50_sg=0_IN-IN                        0.693159   \n_word2vec_mc=10_size=200_window=50_sg=0_IN-OUT                       0.664990   \n_word2vec_mc=10_size=200_window=50_sg=0_OUT-IN                       0.000000   \n_word2vec_mc=10_size=200_window=50_sg=0_OUT-OUT                      0.607646   \n_word2vec_mc=10_size=200_window=5_sg=0_IN-IN                         0.737425   \n_word2vec_mc=10_size=200_window=5_sg=0_IN-OUT                        0.559356   \n_word2vec_mc=10_size=200_window=5_sg=0_OUT-IN                        0.000000   \n_word2vec_mc=10_size=200_window=5_sg=0_OUT-OUT                       0.621730   \n_word2vec_mc=10_size=200_window=5_sg=1_IN-IN                         0.711268   \n_word2vec_mc=10_size=200_window=5_sg=1_IN-OUT                        0.606640   \n_word2vec_mc=10_size=200_window=5_sg=1_OUT-IN                        0.519115   \n_word2vec_mc=10_size=200_window=5_sg=1_OUT-OUT                       0.680080   \n_word2vec_mc=10_size=300_window=5_sg=0_IN-IN                         0.740443   \n_word2vec_mc=10_size=300_window=5_sg=0_IN-OUT                        0.550302   \n_word2vec_mc=10_size=300_window=5_sg=0_OUT-IN                        0.001006   \n_word2vec_mc=10_size=300_window=5_sg=0_OUT-OUT                       0.606640   \n_word2vec_mc=10_size=100_window=50_sg=1_IN-IN                        0.522133   \n_word2vec_mc=10_size=100_window=50_sg=1_IN-OUT                       0.412475   \n_word2vec_mc=10_size=100_window=50_sg=1_OUT-IN                       0.279678   \n_word2vec_mc=10_size=100_window=50_sg=1_OUT-OUT                      0.508048   \n\n                                                 google_analogy_cos3mul_score  \\\n_word2vec_mc=10_size=100_window=50_sg=0_IN-IN                        0.616700   \n_word2vec_mc=10_size=100_window=50_sg=0_IN-OUT                       0.610664   \n_word2vec_mc=10_size=100_window=50_sg=0_OUT-IN                       0.000000   \n_word2vec_mc=10_size=100_window=50_sg=0_OUT-OUT                      0.546278   \n_word2vec_mc=10_size=100_window=5_sg=1_IN-IN                         0.635815   \n_word2vec_mc=10_size=100_window=5_sg=1_IN-OUT                        0.505030   \n_word2vec_mc=10_size=100_window=5_sg=1_OUT-IN                        0.377264   \n_word2vec_mc=10_size=100_window=5_sg=1_OUT-OUT                       0.593561   \n_word2vec_mc=10_size=300_window=5_sg=1_IN-IN                         0.758551   \n_word2vec_mc=10_size=300_window=5_sg=1_IN-OUT                        0.626761   \n_word2vec_mc=10_size=300_window=5_sg=1_OUT-IN                        0.520121   \n_word2vec_mc=10_size=300_window=5_sg=1_OUT-OUT                       0.733400   \n_word2vec_mc=10_size=100_window=5_sg=0_IN-IN                         0.667002   \n_word2vec_mc=10_size=100_window=5_sg=0_IN-OUT                        0.515091   \n_word2vec_mc=10_size=100_window=5_sg=0_OUT-IN                        0.000000   \n_word2vec_mc=10_size=100_window=5_sg=0_OUT-OUT                       0.589537   \n_word2vec_mc=10_size=200_window=50_sg=1_IN-IN                        0.574447   \n_word2vec_mc=10_size=200_window=50_sg=1_IN-OUT                       0.479879   \n_word2vec_mc=10_size=200_window=50_sg=1_OUT-IN                       0.362173   \n_word2vec_mc=10_size=200_window=50_sg=1_OUT-OUT                      0.577465   \n_word2vec_mc=10_size=300_window=50_sg=1_IN-IN                        0.615694   \n_word2vec_mc=10_size=300_window=50_sg=1_IN-OUT                       0.518109   \n_word2vec_mc=10_size=300_window=50_sg=1_OUT-IN                       0.404427   \n_word2vec_mc=10_size=300_window=50_sg=1_OUT-OUT                      0.597586   \n_word2vec_mc=10_size=300_window=50_sg=0_IN-IN                        0.722334   \n_word2vec_mc=10_size=300_window=50_sg=0_IN-OUT                       0.675050   \n_word2vec_mc=10_size=300_window=50_sg=0_OUT-IN                       0.000000   \n_word2vec_mc=10_size=300_window=50_sg=0_OUT-OUT                      0.612676   \n_word2vec_mc=10_size=200_window=50_sg=0_IN-IN                        0.691147   \n_word2vec_mc=10_size=200_window=50_sg=0_IN-OUT                       0.669014   \n_word2vec_mc=10_size=200_window=50_sg=0_OUT-IN                       0.000000   \n_word2vec_mc=10_size=200_window=50_sg=0_OUT-OUT                      0.606640   \n_word2vec_mc=10_size=200_window=5_sg=0_IN-IN                         0.743461   \n_word2vec_mc=10_size=200_window=5_sg=0_IN-OUT                        0.560362   \n_word2vec_mc=10_size=200_window=5_sg=0_OUT-IN                        0.000000   \n_word2vec_mc=10_size=200_window=5_sg=0_OUT-OUT                       0.627767   \n_word2vec_mc=10_size=200_window=5_sg=1_IN-IN                         0.718310   \n_word2vec_mc=10_size=200_window=5_sg=1_IN-OUT                        0.602616   \n_word2vec_mc=10_size=200_window=5_sg=1_OUT-IN                        0.490946   \n_word2vec_mc=10_size=200_window=5_sg=1_OUT-OUT                       0.692153   \n_word2vec_mc=10_size=300_window=5_sg=0_IN-IN                         0.759557   \n_word2vec_mc=10_size=300_window=5_sg=0_IN-OUT                        0.561368   \n_word2vec_mc=10_size=300_window=5_sg=0_OUT-IN                        0.014085   \n_word2vec_mc=10_size=300_window=5_sg=0_OUT-OUT                       0.620724   \n_word2vec_mc=10_size=100_window=50_sg=1_IN-IN                        0.504024   \n_word2vec_mc=10_size=100_window=50_sg=1_IN-OUT                       0.399396   \n_word2vec_mc=10_size=100_window=50_sg=1_OUT-IN                       0.233400   \n_word2vec_mc=10_size=100_window=50_sg=1_OUT-OUT                      0.495976   \n\n                                                 bats_analogy_cos3add_score  \\\n_word2vec_mc=10_size=100_window=50_sg=0_IN-IN                      0.255814   \n_word2vec_mc=10_size=100_window=50_sg=0_IN-OUT                     0.195349   \n_word2vec_mc=10_size=100_window=50_sg=0_OUT-IN                     0.000000   \n_word2vec_mc=10_size=100_window=50_sg=0_OUT-OUT                    0.232558   \n_word2vec_mc=10_size=100_window=5_sg=1_IN-IN                       0.216279   \n_word2vec_mc=10_size=100_window=5_sg=1_IN-OUT                      0.172093   \n_word2vec_mc=10_size=100_window=5_sg=1_OUT-IN                      0.155814   \n_word2vec_mc=10_size=100_window=5_sg=1_OUT-OUT                     0.223256   \n_word2vec_mc=10_size=300_window=5_sg=1_IN-IN                       0.246512   \n_word2vec_mc=10_size=300_window=5_sg=1_IN-OUT                      0.211628   \n_word2vec_mc=10_size=300_window=5_sg=1_OUT-IN                      0.186047   \n_word2vec_mc=10_size=300_window=5_sg=1_OUT-OUT                     0.255814   \n_word2vec_mc=10_size=100_window=5_sg=0_IN-IN                       0.272093   \n_word2vec_mc=10_size=100_window=5_sg=0_IN-OUT                      0.153488   \n_word2vec_mc=10_size=100_window=5_sg=0_OUT-IN                      0.000000   \n_word2vec_mc=10_size=100_window=5_sg=0_OUT-OUT                     0.206977   \n_word2vec_mc=10_size=200_window=50_sg=1_IN-IN                      0.204651   \n_word2vec_mc=10_size=200_window=50_sg=1_IN-OUT                     0.130233   \n_word2vec_mc=10_size=200_window=50_sg=1_OUT-IN                     0.118605   \n_word2vec_mc=10_size=200_window=50_sg=1_OUT-OUT                    0.190698   \n_word2vec_mc=10_size=300_window=50_sg=1_IN-IN                      0.211628   \n_word2vec_mc=10_size=300_window=50_sg=1_IN-OUT                     0.167442   \n_word2vec_mc=10_size=300_window=50_sg=1_OUT-IN                     0.137209   \n_word2vec_mc=10_size=300_window=50_sg=1_OUT-OUT                    0.211628   \n_word2vec_mc=10_size=300_window=50_sg=0_IN-IN                      0.267442   \n_word2vec_mc=10_size=300_window=50_sg=0_IN-OUT                     0.227907   \n_word2vec_mc=10_size=300_window=50_sg=0_OUT-IN                     0.000000   \n_word2vec_mc=10_size=300_window=50_sg=0_OUT-OUT                    0.202326   \n_word2vec_mc=10_size=200_window=50_sg=0_IN-IN                      0.272093   \n_word2vec_mc=10_size=200_window=50_sg=0_IN-OUT                     0.220930   \n_word2vec_mc=10_size=200_window=50_sg=0_OUT-IN                     0.000000   \n_word2vec_mc=10_size=200_window=50_sg=0_OUT-OUT                    0.216279   \n_word2vec_mc=10_size=200_window=5_sg=0_IN-IN                       0.276744   \n_word2vec_mc=10_size=200_window=5_sg=0_IN-OUT                      0.144186   \n_word2vec_mc=10_size=200_window=5_sg=0_OUT-IN                      0.000000   \n_word2vec_mc=10_size=200_window=5_sg=0_OUT-OUT                     0.188372   \n_word2vec_mc=10_size=200_window=5_sg=1_IN-IN                       0.251163   \n_word2vec_mc=10_size=200_window=5_sg=1_IN-OUT                      0.206977   \n_word2vec_mc=10_size=200_window=5_sg=1_OUT-IN                      0.172093   \n_word2vec_mc=10_size=200_window=5_sg=1_OUT-OUT                     0.248837   \n_word2vec_mc=10_size=300_window=5_sg=0_IN-IN                       0.281395   \n_word2vec_mc=10_size=300_window=5_sg=0_IN-OUT                      0.167442   \n_word2vec_mc=10_size=300_window=5_sg=0_OUT-IN                      0.000000   \n_word2vec_mc=10_size=300_window=5_sg=0_OUT-OUT                     0.183721   \n_word2vec_mc=10_size=100_window=50_sg=1_IN-IN                      0.167442   \n_word2vec_mc=10_size=100_window=50_sg=1_IN-OUT                     0.118605   \n_word2vec_mc=10_size=100_window=50_sg=1_OUT-IN                     0.060465   \n_word2vec_mc=10_size=100_window=50_sg=1_OUT-OUT                    0.162791   \n\n                                                 bats_analogy_cos3mul_score  \n_word2vec_mc=10_size=100_window=50_sg=0_IN-IN                      0.200000  \n_word2vec_mc=10_size=100_window=50_sg=0_IN-OUT                     0.186047  \n_word2vec_mc=10_size=100_window=50_sg=0_OUT-IN                     0.000000  \n_word2vec_mc=10_size=100_window=50_sg=0_OUT-OUT                    0.234884  \n_word2vec_mc=10_size=100_window=5_sg=1_IN-IN                       0.211628  \n_word2vec_mc=10_size=100_window=5_sg=1_IN-OUT                      0.160465  \n_word2vec_mc=10_size=100_window=5_sg=1_OUT-IN                      0.123256  \n_word2vec_mc=10_size=100_window=5_sg=1_OUT-OUT                     0.213953  \n_word2vec_mc=10_size=300_window=5_sg=1_IN-IN                       0.267442  \n_word2vec_mc=10_size=300_window=5_sg=1_IN-OUT                      0.209302  \n_word2vec_mc=10_size=300_window=5_sg=1_OUT-IN                      0.179070  \n_word2vec_mc=10_size=300_window=5_sg=1_OUT-OUT                     0.283721  \n_word2vec_mc=10_size=100_window=5_sg=0_IN-IN                       0.230233  \n_word2vec_mc=10_size=100_window=5_sg=0_IN-OUT                      0.151163  \n_word2vec_mc=10_size=100_window=5_sg=0_OUT-IN                      0.000000  \n_word2vec_mc=10_size=100_window=5_sg=0_OUT-OUT                     0.204651  \n_word2vec_mc=10_size=200_window=50_sg=1_IN-IN                      0.190698  \n_word2vec_mc=10_size=200_window=50_sg=1_IN-OUT                     0.134884  \n_word2vec_mc=10_size=200_window=50_sg=1_OUT-IN                     0.104651  \n_word2vec_mc=10_size=200_window=50_sg=1_OUT-OUT                    0.188372  \n_word2vec_mc=10_size=300_window=50_sg=1_IN-IN                      0.211628  \n_word2vec_mc=10_size=300_window=50_sg=1_IN-OUT                     0.160465  \n_word2vec_mc=10_size=300_window=50_sg=1_OUT-IN                     0.125581  \n_word2vec_mc=10_size=300_window=50_sg=1_OUT-OUT                    0.202326  \n_word2vec_mc=10_size=300_window=50_sg=0_IN-IN                      0.267442  \n_word2vec_mc=10_size=300_window=50_sg=0_IN-OUT                     0.232558  \n_word2vec_mc=10_size=300_window=50_sg=0_OUT-IN                     0.000000  \n_word2vec_mc=10_size=300_window=50_sg=0_OUT-OUT                    0.209302  \n_word2vec_mc=10_size=200_window=50_sg=0_IN-IN                      0.272093  \n_word2vec_mc=10_size=200_window=50_sg=0_IN-OUT                     0.225581  \n_word2vec_mc=10_size=200_window=50_sg=0_OUT-IN                     0.000000  \n_word2vec_mc=10_size=200_window=50_sg=0_OUT-OUT                    0.213953  \n_word2vec_mc=10_size=200_window=5_sg=0_IN-IN                       0.265116  \n_word2vec_mc=10_size=200_window=5_sg=0_IN-OUT                      0.148837  \n_word2vec_mc=10_size=200_window=5_sg=0_OUT-IN                      0.000000  \n_word2vec_mc=10_size=200_window=5_sg=0_OUT-OUT                     0.193023  \n_word2vec_mc=10_size=200_window=5_sg=1_IN-IN                       0.265116  \n_word2vec_mc=10_size=200_window=5_sg=1_IN-OUT                      0.190698  \n_word2vec_mc=10_size=200_window=5_sg=1_OUT-IN                      0.162791  \n_word2vec_mc=10_size=200_window=5_sg=1_OUT-OUT                     0.241860  \n_word2vec_mc=10_size=300_window=5_sg=0_IN-IN                       0.297674  \n_word2vec_mc=10_size=300_window=5_sg=0_IN-OUT                      0.167442  \n_word2vec_mc=10_size=300_window=5_sg=0_OUT-IN                      0.009302  \n_word2vec_mc=10_size=300_window=5_sg=0_OUT-OUT                     0.188372  \n_word2vec_mc=10_size=100_window=50_sg=1_IN-IN                      0.141860  \n_word2vec_mc=10_size=100_window=50_sg=1_IN-OUT                     0.102326  \n_word2vec_mc=10_size=100_window=50_sg=1_OUT-IN                     0.046512  \n_word2vec_mc=10_size=100_window=50_sg=1_OUT-OUT                    0.151163  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>google_analogy_cos3add_score</th>\n      <th>google_analogy_cos3mul_score</th>\n      <th>bats_analogy_cos3add_score</th>\n      <th>bats_analogy_cos3mul_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=50_sg=0_IN-IN</th>\n      <td>0.667002</td>\n      <td>0.616700</td>\n      <td>0.255814</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=50_sg=0_IN-OUT</th>\n      <td>0.620724</td>\n      <td>0.610664</td>\n      <td>0.195349</td>\n      <td>0.186047</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=50_sg=0_OUT-IN</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=50_sg=0_OUT-OUT</th>\n      <td>0.546278</td>\n      <td>0.546278</td>\n      <td>0.232558</td>\n      <td>0.234884</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=5_sg=1_IN-IN</th>\n      <td>0.646881</td>\n      <td>0.635815</td>\n      <td>0.216279</td>\n      <td>0.211628</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=5_sg=1_IN-OUT</th>\n      <td>0.537223</td>\n      <td>0.505030</td>\n      <td>0.172093</td>\n      <td>0.160465</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=5_sg=1_OUT-IN</th>\n      <td>0.440644</td>\n      <td>0.377264</td>\n      <td>0.155814</td>\n      <td>0.123256</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=5_sg=1_OUT-OUT</th>\n      <td>0.608652</td>\n      <td>0.593561</td>\n      <td>0.223256</td>\n      <td>0.213953</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=5_sg=1_IN-IN</th>\n      <td>0.732394</td>\n      <td>0.758551</td>\n      <td>0.246512</td>\n      <td>0.267442</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=5_sg=1_IN-OUT</th>\n      <td>0.628773</td>\n      <td>0.626761</td>\n      <td>0.211628</td>\n      <td>0.209302</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=5_sg=1_OUT-IN</th>\n      <td>0.529175</td>\n      <td>0.520121</td>\n      <td>0.186047</td>\n      <td>0.179070</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=5_sg=1_OUT-OUT</th>\n      <td>0.703219</td>\n      <td>0.733400</td>\n      <td>0.255814</td>\n      <td>0.283721</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=5_sg=0_IN-IN</th>\n      <td>0.701207</td>\n      <td>0.667002</td>\n      <td>0.272093</td>\n      <td>0.230233</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=5_sg=0_IN-OUT</th>\n      <td>0.520121</td>\n      <td>0.515091</td>\n      <td>0.153488</td>\n      <td>0.151163</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=5_sg=0_OUT-IN</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=5_sg=0_OUT-OUT</th>\n      <td>0.590543</td>\n      <td>0.589537</td>\n      <td>0.206977</td>\n      <td>0.204651</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=50_sg=1_IN-IN</th>\n      <td>0.574447</td>\n      <td>0.574447</td>\n      <td>0.204651</td>\n      <td>0.190698</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=50_sg=1_IN-OUT</th>\n      <td>0.487928</td>\n      <td>0.479879</td>\n      <td>0.130233</td>\n      <td>0.134884</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=50_sg=1_OUT-IN</th>\n      <td>0.387324</td>\n      <td>0.362173</td>\n      <td>0.118605</td>\n      <td>0.104651</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=50_sg=1_OUT-OUT</th>\n      <td>0.559356</td>\n      <td>0.577465</td>\n      <td>0.190698</td>\n      <td>0.188372</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=50_sg=1_IN-IN</th>\n      <td>0.613682</td>\n      <td>0.615694</td>\n      <td>0.211628</td>\n      <td>0.211628</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=50_sg=1_IN-OUT</th>\n      <td>0.517103</td>\n      <td>0.518109</td>\n      <td>0.167442</td>\n      <td>0.160465</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=50_sg=1_OUT-IN</th>\n      <td>0.427565</td>\n      <td>0.404427</td>\n      <td>0.137209</td>\n      <td>0.125581</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=50_sg=1_OUT-OUT</th>\n      <td>0.589537</td>\n      <td>0.597586</td>\n      <td>0.211628</td>\n      <td>0.202326</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=50_sg=0_IN-IN</th>\n      <td>0.708249</td>\n      <td>0.722334</td>\n      <td>0.267442</td>\n      <td>0.267442</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=50_sg=0_IN-OUT</th>\n      <td>0.663984</td>\n      <td>0.675050</td>\n      <td>0.227907</td>\n      <td>0.232558</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=50_sg=0_OUT-IN</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=50_sg=0_OUT-OUT</th>\n      <td>0.606640</td>\n      <td>0.612676</td>\n      <td>0.202326</td>\n      <td>0.209302</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=50_sg=0_IN-IN</th>\n      <td>0.693159</td>\n      <td>0.691147</td>\n      <td>0.272093</td>\n      <td>0.272093</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=50_sg=0_IN-OUT</th>\n      <td>0.664990</td>\n      <td>0.669014</td>\n      <td>0.220930</td>\n      <td>0.225581</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=50_sg=0_OUT-IN</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=50_sg=0_OUT-OUT</th>\n      <td>0.607646</td>\n      <td>0.606640</td>\n      <td>0.216279</td>\n      <td>0.213953</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=5_sg=0_IN-IN</th>\n      <td>0.737425</td>\n      <td>0.743461</td>\n      <td>0.276744</td>\n      <td>0.265116</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=5_sg=0_IN-OUT</th>\n      <td>0.559356</td>\n      <td>0.560362</td>\n      <td>0.144186</td>\n      <td>0.148837</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=5_sg=0_OUT-IN</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=5_sg=0_OUT-OUT</th>\n      <td>0.621730</td>\n      <td>0.627767</td>\n      <td>0.188372</td>\n      <td>0.193023</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=5_sg=1_IN-IN</th>\n      <td>0.711268</td>\n      <td>0.718310</td>\n      <td>0.251163</td>\n      <td>0.265116</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=5_sg=1_IN-OUT</th>\n      <td>0.606640</td>\n      <td>0.602616</td>\n      <td>0.206977</td>\n      <td>0.190698</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=5_sg=1_OUT-IN</th>\n      <td>0.519115</td>\n      <td>0.490946</td>\n      <td>0.172093</td>\n      <td>0.162791</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=200_window=5_sg=1_OUT-OUT</th>\n      <td>0.680080</td>\n      <td>0.692153</td>\n      <td>0.248837</td>\n      <td>0.241860</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=5_sg=0_IN-IN</th>\n      <td>0.740443</td>\n      <td>0.759557</td>\n      <td>0.281395</td>\n      <td>0.297674</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=5_sg=0_IN-OUT</th>\n      <td>0.550302</td>\n      <td>0.561368</td>\n      <td>0.167442</td>\n      <td>0.167442</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=5_sg=0_OUT-IN</th>\n      <td>0.001006</td>\n      <td>0.014085</td>\n      <td>0.000000</td>\n      <td>0.009302</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=300_window=5_sg=0_OUT-OUT</th>\n      <td>0.606640</td>\n      <td>0.620724</td>\n      <td>0.183721</td>\n      <td>0.188372</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=50_sg=1_IN-IN</th>\n      <td>0.522133</td>\n      <td>0.504024</td>\n      <td>0.167442</td>\n      <td>0.141860</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=50_sg=1_IN-OUT</th>\n      <td>0.412475</td>\n      <td>0.399396</td>\n      <td>0.118605</td>\n      <td>0.102326</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=50_sg=1_OUT-IN</th>\n      <td>0.279678</td>\n      <td>0.233400</td>\n      <td>0.060465</td>\n      <td>0.046512</td>\n    </tr>\n    <tr>\n      <th>_word2vec_mc=10_size=100_window=50_sg=1_OUT-OUT</th>\n      <td>0.508048</td>\n      <td>0.495976</td>\n      <td>0.162791</td>\n      <td>0.151163</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00043-9caf419b-8c5c-4c6d-8d13-431c546e60c7"},"source":"import re\nre.findall( \"_word2vec.*\", glob.glob(\"../output/word2vec/analogy/*\")[0])","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":134,"data":{"text/plain":"['_word2vec_mc=10_size=100_window=50_sg=0_analogy.pickle']"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00044-f0cd999a-a174-44bd-ab89-78f6bf31afea"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"deepnote_execution_queue":[{"cellId":"00021-d2d51152-fe26-4a66-8e51-8ce8c479de9f","sessionId":"f4efa773-16e6-44de-921a-9e0f494d27ff","msgId":"468957f4-0ec0-44f2-8f7a-1f5a822ac70e"}],"deepnote_notebook_id":"ee79aacb-fd8f-4052-af8b-c77445857e56","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}}}