{"cells":[{"cell_type":"markdown","source":"# Word2Vec models comparison\n\nWe compare the different models of word2vec against different intrinsic word embeddings tasks.","metadata":{"tags":[],"cell_id":"00001-041ba339-031d-426b-8697-9dda6244991a"}},{"cell_type":"markdown","source":"### Import and load datasets","metadata":{"tags":[],"cell_id":"00001-7d1cb22c-84e3-419d-8987-607100f4798d"}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"tags":[],"cell_id":"00002-2ea9b422-e508-4ea8-b4d1-db67488afc69"},"outputs":[],"execution_count":33},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-3b9bb0a0-e60c-4aee-8287-c11f6e167fc0"},"source":"# imports\nimport glob\nimport pandas as pd\n!pip install gensim\nfrom gensim.models import KeyedVectors\nfrom gensim.models.word2vec import Word2Vec\n\n# load the files\ndef load_similarity_datasets():\n    \"\"\"Load all (13) datasets which can be sued to test word interchangeable similarity\n    \"\"\"\n    sim_data = {}\n    for file_path in glob.glob(\"../data/word-sim/*\"):\n        file_name = file_path[17:].replace(\".txt\", \"\")\n        print(file_name)\n        try:\n            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n            df.columns = ['word_1', 'word_2', 'similarity_score']\n        except:\n            df = pd.read_csv(file_path, sep=\" \", header=None)\n            df.columns = ['word_1', 'word_2', 'similarity_score']\n        sim_data[file_name] = df\n    return sim_data\n\n# load similarity datasets\nsimilarity_datasets = load_similarity_datasets()","execution_count":23,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gensim in /opt/venv/lib/python3.7/site-packages (3.8.3)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/venv/lib/python3.7/site-packages (from gensim) (2.1.0)\nRequirement already satisfied: scipy>=0.18.1 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.4.1)\nRequirement already satisfied: numpy>=1.11.3 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.19.0)\nRequirement already satisfied: six>=1.5.0 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.15.0)\nRequirement already satisfied: boto in /opt/venv/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\nRequirement already satisfied: boto3 in /opt/venv/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.14.41)\nRequirement already satisfied: requests in /opt/venv/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/venv/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/venv/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\nRequirement already satisfied: botocore<1.18.0,>=1.17.41 in /opt/venv/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.41)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/venv/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.41->boto3->smart-open>=1.8.1->gensim) (0.15.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/venv/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.41->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nEN-VERB-143\nEN-SimVerb-3500\nEN-RG-65\nEN-RW-STANFORD\nEN-MTurk-771\nEN-MEN-TR-3k\nEN-MC-30\nEN-MTurk-287\nEN-SIMLEX-999\nEN-WS-353-REL\nEN-YP-130\nEN-WS-353-ALL\nEN-WS-353-SIM\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load word2vec models","metadata":{"tags":[],"cell_id":"00004-2cbf3bf3-975e-41b3-9b4c-b2a8368233e2"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-c06d8002-eee2-4903-a5fc-6e3ef459cc0c"},"source":"model = Word2Vec.load(\"../../../embeddings_lemma/word2vec_mc=10_iter=5_size=100_window=5_sg=0/word2vec_wikiEn20171001_millionSentences_mc=10_iter=5_size=100_window=5_sg=0\")","execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Create Word2vec similarity computing method","metadata":{"tags":[],"cell_id":"00005-f46a97c1-8bd1-4d13-af3e-b05a52d2f97d"}},{"cell_type":"code","source":"def word2vec_get_index_by_word(model, word):\n    \"\"\"Return the index of the word in the model\n    \"\"\"\n    return model.wv.index2word.index(word)\n\ndef word2vec_get_word_by_index(model, index):\n    \"\"\"Return the word by the provided index\n    \"\"\"\n    return model.wv.index2word[index]\n\ndef word2vec_find_top_similar_words(model, source_word, method='IN-IN', top_n=5, no_self_similarity=True):\n    \"\"\"\n    Provided a word, find the top_n most similar from the model following the method\n    \"\"\"\n    score = []\n    input_weights = model.wv.syn0\n    output_weights = model.syn1neg\n    source_word_index = word2vec_get_index_by_word(model, source_word)\n    if method==\"IN-IN\":\n        weights1, weights2 = input_weights, input_weights\n    elif method==\"IN-OUT\":\n        weights1, weights2 = input_weights, output_weights\n    elif method==\"OUT-IN\":\n        weights1, weights2 = output_weights, input_weights\n    elif method==\"OUT-OUT\":\n        weights1, weights2 = output_weights, output_weights\n    score = cosine_similarity(weights1[source_word_index].reshape(1, -1), weights2)[0]\n    if no_self_similarity:\n        score[source_word_index] = -1 # negate self-similarity\n    top_n_similar_words = np.argpartition(-score, top_n)[:top_n]\n    return sorted([(word2vec_get_word_by_index(model, index), score[index]) for index in top_n_similar_words], \n                key=lambda x: x[1], \n                reverse=True)\n\ndef word2vec_find_similarity(model, source_word, target_word, method=\"IN-IN\"):\n    \"\"\"Return the cosine similarity between two words based on the suggested method\n    \"\"\"\n    input_weights = model.wv.syn0\n    output_weights = model.syn1neg\n    source_word_index = word2vec_get_index_by_word(model, source_word)\n    target_word_index = word2vec_get_index_by_word(model, target_word)\n    if method==\"IN-IN\":\n        weights1, weights2 = input_weights, input_weights\n    elif method==\"IN-OUT\":\n        weights1, weights2 = input_weights, output_weights\n    elif method==\"OUT-IN\":\n        weights1, weights2 = output_weights, input_weights\n    elif method==\"OUT-OUT\":\n        weights1, weights2 = output_weights, output_weights\n    score = cosine_similarity(weights1[source_word_index].reshape(1, -1), \n                              weights2[target_word_index].reshape(1, -1))[0]\n    return score\n# word2vec_find_similarity(model, \"car\", \"truck\", \"IN-OUT\")\n# word2vec_find_top_similar_words(model, \"car\", \"IN-IN\")\n# word2vec_find_top_similar_words(model, \"car\", \"IN-OUT\")","metadata":{"tags":[],"cell_id":"00009-3634ec7a-94d6-4af2-b17b-978878063815"},"outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:38: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n/opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `syn1neg` (Attribute will be removed in 4.0.0, use self.trainables.syn1neg instead).\n/opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n  app.launch_new_instance()\n/opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `syn1neg` (Attribute will be removed in 4.0.0, use self.trainables.syn1neg instead).\n","output_type":"stream"},{"output_type":"execute_result","execution_count":78,"data":{"text/plain":"[('motorcycle', 0.1590093),\n ('racing', 0.15262602),\n ('driver', 0.14650589),\n ('truck', 0.14469106),\n ('motor', 0.14405525)]"},"metadata":{}}],"execution_count":78},{"cell_type":"markdown","source":"### Generate stats for each similarity dataset","metadata":{"tags":[],"cell_id":"00008-1fa7bc48-1878-4183-a8ca-6ec498708ca8"}},{"cell_type":"code","source":"%%capture\ndf = similarity_datasets['EN-SIMLEX-999'].copy()\nscore_table = []\ndimension = model.syn1neg.shape[1]\nfor row in df.to_dict(orient=\"records\"):\n    methods = [\"IN-IN\", \"IN-OUT\", \"OUT-IN\", \"OUT-OUT\"]\n    for method in methods:\n        try:\n            sim_score = word2vec_find_similarity(model, row['word_1'], row['word_2'], method)[0]\n        except:\n            sim_score = None\n        row[f'word2vec_{dimension}_{method}_sim_score'] = sim_score\n    score_table.append(row)\nscore_table = pd.DataFrame.from_dict(score_table)","metadata":{"tags":[],"cell_id":"00010-fe50b5c5-70eb-448c-882b-f8d7e0dd50e5"},"outputs":[],"execution_count":106},{"cell_type":"code","source":"score_table.dropna().corr(\"pearson\")","metadata":{"tags":[],"cell_id":"00011-9fe7537e-ee70-43b2-8b4a-50cc5839c0e2"},"outputs":[{"output_type":"execute_result","execution_count":110,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":5,"columns":[{"name":"similarity_score","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":0.26776649163249067,"max":1,"histogram":[{"bin_start":0.26776649163249067,"bin_end":0.3409898424692416,"count":3},{"bin_start":0.3409898424692416,"bin_end":0.41421319330599254,"count":1},{"bin_start":0.41421319330599254,"bin_end":0.4874365441427435,"count":0},{"bin_start":0.4874365441427435,"bin_end":0.5606598949794944,"count":0},{"bin_start":0.5606598949794944,"bin_end":0.6338832458162453,"count":0},{"bin_start":0.6338832458162453,"bin_end":0.7071065966529964,"count":0},{"bin_start":0.7071065966529964,"bin_end":0.7803299474897473,"count":0},{"bin_start":0.7803299474897473,"bin_end":0.8535532983264982,"count":0},{"bin_start":0.8535532983264982,"bin_end":0.9267766491632491,"count":0},{"bin_start":0.9267766491632491,"bin_end":1,"count":1}]}},{"name":"word2vec_100_IN-IN_sim_score","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":0.378299293895258,"max":1,"histogram":[{"bin_start":0.378299293895258,"bin_end":0.4404693645057322,"count":1},{"bin_start":0.4404693645057322,"bin_end":0.5026394351162065,"count":0},{"bin_start":0.5026394351162065,"bin_end":0.5648095057266806,"count":0},{"bin_start":0.5648095057266806,"bin_end":0.6269795763371548,"count":0},{"bin_start":0.6269795763371548,"bin_end":0.689149646947629,"count":2},{"bin_start":0.689149646947629,"bin_end":0.7513197175581032,"count":0},{"bin_start":0.7513197175581032,"bin_end":0.8134897881685774,"count":1},{"bin_start":0.8134897881685774,"bin_end":0.8756598587790516,"count":0},{"bin_start":0.8756598587790516,"bin_end":0.9378299293895258,"count":0},{"bin_start":0.9378299293895258,"bin_end":1,"count":1}]}},{"name":"word2vec_100_IN-OUT_sim_score","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":0.28433528941977404,"max":1,"histogram":[{"bin_start":0.28433528941977404,"bin_end":0.35590176047779665,"count":1},{"bin_start":0.35590176047779665,"bin_end":0.42746823153581925,"count":0},{"bin_start":0.42746823153581925,"bin_end":0.4990347025938418,"count":1},{"bin_start":0.4990347025938418,"bin_end":0.5706011736518644,"count":0},{"bin_start":0.5706011736518644,"bin_end":0.642167644709887,"count":1},{"bin_start":0.642167644709887,"bin_end":0.7137341157679096,"count":0},{"bin_start":0.7137341157679096,"bin_end":0.7853005868259322,"count":0},{"bin_start":0.7853005868259322,"bin_end":0.8568670578839548,"count":0},{"bin_start":0.8568670578839548,"bin_end":0.9284335289419774,"count":1},{"bin_start":0.9284335289419774,"bin_end":1,"count":1}]}},{"name":"word2vec_100_OUT-IN_sim_score","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":0.26776649163249067,"max":1,"histogram":[{"bin_start":0.26776649163249067,"bin_end":0.3409898424692416,"count":1},{"bin_start":0.3409898424692416,"bin_end":0.41421319330599254,"count":0},{"bin_start":0.41421319330599254,"bin_end":0.4874365441427435,"count":1},{"bin_start":0.4874365441427435,"bin_end":0.5606598949794944,"count":0},{"bin_start":0.5606598949794944,"bin_end":0.6338832458162453,"count":0},{"bin_start":0.6338832458162453,"bin_end":0.7071065966529964,"count":1},{"bin_start":0.7071065966529964,"bin_end":0.7803299474897473,"count":0},{"bin_start":0.7803299474897473,"bin_end":0.8535532983264982,"count":0},{"bin_start":0.8535532983264982,"bin_end":0.9267766491632491,"count":1},{"bin_start":0.9267766491632491,"bin_end":1,"count":1}]}},{"name":"word2vec_100_OUT-OUT_sim_score","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":0.3268358962361852,"max":1,"histogram":[{"bin_start":0.3268358962361852,"bin_end":0.39415230661256667,"count":1},{"bin_start":0.39415230661256667,"bin_end":0.46146871698894815,"count":0},{"bin_start":0.46146871698894815,"bin_end":0.5287851273653297,"count":2},{"bin_start":0.5287851273653297,"bin_end":0.5961015377417112,"count":0},{"bin_start":0.5961015377417112,"bin_end":0.6634179481180926,"count":0},{"bin_start":0.6634179481180926,"bin_end":0.7307343584944741,"count":0},{"bin_start":0.7307343584944741,"bin_end":0.7980507688708556,"count":1},{"bin_start":0.7980507688708556,"bin_end":0.865367179247237,"count":0},{"bin_start":0.865367179247237,"bin_end":0.9326835896236185,"count":0},{"bin_start":0.9326835896236185,"bin_end":1,"count":1}]}},{"name":"_deepnote_index_column","dtype":"object"}],"rows_top":[{"similarity_score":1,"word2vec_100_IN-IN_sim_score":0.378299293895258,"word2vec_100_IN-OUT_sim_score":0.28433528941977404,"word2vec_100_OUT-IN_sim_score":0.26776649163249067,"word2vec_100_OUT-OUT_sim_score":0.3268358962361852,"_deepnote_index_column":"similarity_score"},{"similarity_score":0.378299293895258,"word2vec_100_IN-IN_sim_score":1,"word2vec_100_IN-OUT_sim_score":0.6290711764425195,"word2vec_100_OUT-IN_sim_score":0.64211004201536,"word2vec_100_OUT-OUT_sim_score":0.7906238382155103,"_deepnote_index_column":"word2vec_100_IN-IN_sim_score"},{"similarity_score":0.28433528941977404,"word2vec_100_IN-IN_sim_score":0.6290711764425195,"word2vec_100_IN-OUT_sim_score":1,"word2vec_100_OUT-IN_sim_score":0.9209477013494461,"word2vec_100_OUT-OUT_sim_score":0.4752112857861825,"_deepnote_index_column":"word2vec_100_IN-OUT_sim_score"},{"similarity_score":0.26776649163249067,"word2vec_100_IN-IN_sim_score":0.64211004201536,"word2vec_100_IN-OUT_sim_score":0.9209477013494461,"word2vec_100_OUT-IN_sim_score":1,"word2vec_100_OUT-OUT_sim_score":0.4751503500242323,"_deepnote_index_column":"word2vec_100_OUT-IN_sim_score"},{"similarity_score":0.3268358962361852,"word2vec_100_IN-IN_sim_score":0.7906238382155103,"word2vec_100_IN-OUT_sim_score":0.4752112857861825,"word2vec_100_OUT-IN_sim_score":0.4751503500242323,"word2vec_100_OUT-OUT_sim_score":1,"_deepnote_index_column":"word2vec_100_OUT-OUT_sim_score"}],"rows_bottom":null},"text/plain":"                                similarity_score  \\\nsimilarity_score                        1.000000   \nword2vec_100_IN-IN_sim_score            0.378299   \nword2vec_100_IN-OUT_sim_score           0.284335   \nword2vec_100_OUT-IN_sim_score           0.267766   \nword2vec_100_OUT-OUT_sim_score          0.326836   \n\n                                word2vec_100_IN-IN_sim_score  \\\nsimilarity_score                                    0.378299   \nword2vec_100_IN-IN_sim_score                        1.000000   \nword2vec_100_IN-OUT_sim_score                       0.629071   \nword2vec_100_OUT-IN_sim_score                       0.642110   \nword2vec_100_OUT-OUT_sim_score                      0.790624   \n\n                                word2vec_100_IN-OUT_sim_score  \\\nsimilarity_score                                     0.284335   \nword2vec_100_IN-IN_sim_score                         0.629071   \nword2vec_100_IN-OUT_sim_score                        1.000000   \nword2vec_100_OUT-IN_sim_score                        0.920948   \nword2vec_100_OUT-OUT_sim_score                       0.475211   \n\n                                word2vec_100_OUT-IN_sim_score  \\\nsimilarity_score                                     0.267766   \nword2vec_100_IN-IN_sim_score                         0.642110   \nword2vec_100_IN-OUT_sim_score                        0.920948   \nword2vec_100_OUT-IN_sim_score                        1.000000   \nword2vec_100_OUT-OUT_sim_score                       0.475150   \n\n                                word2vec_100_OUT-OUT_sim_score  \nsimilarity_score                                      0.326836  \nword2vec_100_IN-IN_sim_score                          0.790624  \nword2vec_100_IN-OUT_sim_score                         0.475211  \nword2vec_100_OUT-IN_sim_score                         0.475150  \nword2vec_100_OUT-OUT_sim_score                        1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>similarity_score</th>\n      <th>word2vec_100_IN-IN_sim_score</th>\n      <th>word2vec_100_IN-OUT_sim_score</th>\n      <th>word2vec_100_OUT-IN_sim_score</th>\n      <th>word2vec_100_OUT-OUT_sim_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>similarity_score</th>\n      <td>1.000000</td>\n      <td>0.378299</td>\n      <td>0.284335</td>\n      <td>0.267766</td>\n      <td>0.326836</td>\n    </tr>\n    <tr>\n      <th>word2vec_100_IN-IN_sim_score</th>\n      <td>0.378299</td>\n      <td>1.000000</td>\n      <td>0.629071</td>\n      <td>0.642110</td>\n      <td>0.790624</td>\n    </tr>\n    <tr>\n      <th>word2vec_100_IN-OUT_sim_score</th>\n      <td>0.284335</td>\n      <td>0.629071</td>\n      <td>1.000000</td>\n      <td>0.920948</td>\n      <td>0.475211</td>\n    </tr>\n    <tr>\n      <th>word2vec_100_OUT-IN_sim_score</th>\n      <td>0.267766</td>\n      <td>0.642110</td>\n      <td>0.920948</td>\n      <td>1.000000</td>\n      <td>0.475150</td>\n    </tr>\n    <tr>\n      <th>word2vec_100_OUT-OUT_sim_score</th>\n      <td>0.326836</td>\n      <td>0.790624</td>\n      <td>0.475211</td>\n      <td>0.475150</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":110},{"cell_type":"code","source":"# score_table.isnull().sum()","metadata":{"tags":[],"cell_id":"00013-71c4b9cf-87d5-4c05-9d3f-3ccabc9abc4e"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"3bec70f7-4277-4683-a3e5-692e28462d6a","deepnote_execution_queue":[]}}